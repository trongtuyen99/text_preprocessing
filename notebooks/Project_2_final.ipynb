{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI147aVTYZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "26d6364a-5ac1-4bca-cee5-c8dc7477f94a"
      },
      "source": [
        "# !git clone https://trongtuyen99:password@github.com/trongtuyen99/20news_goup_data.git\n",
        "!git clone https://github.com/trongtuyen99/20news_goup_data.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '20news_goup_data'...\n",
            "remote: Enumerating objects: 37708, done.\u001b[K\n",
            "remote: Counting objects: 100% (37708/37708), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37638/37638), done.\u001b[K\n",
            "remote: Total 37708 (delta 67), reused 37699 (delta 64), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (37708/37708), 30.21 MiB | 21.92 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "Checking out files: 100% (37622/37622), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYveY0QWfuL",
        "colab_type": "text"
      },
      "source": [
        "# import required lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Qy1PEtTEH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "45f47a1b-e16b-432b-d3a6-79244f834001"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "nltk.download('wordnet') # for steemmerize\n",
        "nltk.download('stopwords') # for remove stops word\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "from time import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH-ZKev1S2yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Prepro():\n",
        "    \"\"\"using framework\"\"\"\n",
        "    def __init__(self, min_df):\n",
        "        self.stop_words = stopwords.words(\"english\")\n",
        "        self.min_df = min_df\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def remove_stop_words(self, text):\n",
        "        for word in self.stop_words:\n",
        "                text = text.replace(\" \"+word+\" \", \" \")\n",
        "        return text\n",
        "\n",
        "    def get_words(self, text):\n",
        "        return simple_preprocess(text)\n",
        "\n",
        "    def stem(self, word):\n",
        "      return self.stemmer.stem(word)\n",
        "\n",
        "    def create_dict_from_array(self, doc): # not using\n",
        "      \"\"\"doc: list of list word\"\"\"\n",
        "      dict_corpus = defaultdict(int)\n",
        "      for d in doc:\n",
        "          for w in d:\n",
        "              dict_corpus[w] += 1\n",
        "      return dict_corpus\n",
        "    \n",
        "    def get_data_from_dir(self, path):\n",
        "        \"return list of doc \"\n",
        "        all_files = []\n",
        "        data = []\n",
        "        for root, dirs, files in os.walk(path, topdown=True):\n",
        "            for name in files:\n",
        "                all_files.append(os.path.join(root, name))\n",
        "    \n",
        "        for file in all_files:\n",
        "          with open(file, 'r') as reader:\n",
        "            try:\n",
        "              text = reader.read()\n",
        "              data.append(text)\n",
        "            except:\n",
        "              pass\n",
        "        return data\n",
        "\n",
        "    def process(self, sentence):\n",
        "      text = self.remove_stop_words(sentence)\n",
        "      list_words = self.get_words(text)\n",
        "      word_stem = [my_prepro.stem(w) for w in list_words]\n",
        "      result = \" \".join(word_stem)\n",
        "    \n",
        "    def save_serialize(obj, file_path):\n",
        "      joblib.dump(obj, file_path)\n",
        "\n",
        "    def create_word_freq(self, list_text):\n",
        "      \"\"\" list text: list of paragraph\"\"\"\n",
        "      word_freq = defaultdict(int) \n",
        "      word_doc_freq = defaultdict(int)\n",
        "      n = len(list_text)\n",
        "      for s in list_text:\n",
        "        words = s.split()\n",
        "        for word in words:\n",
        "          word_freq[word] += 1\n",
        "        set_words = set(words)\n",
        "        for word in set_words:\n",
        "          word_doc_freq[word] += 1\n",
        "      for k in word_doc_freq.keys():\n",
        "        word_doc_freq[k] /= n\n",
        "      return word_freq, word_doc_freq"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgPPwMPJpUn2",
        "colab_type": "text"
      },
      "source": [
        "# remove stopword, special, redundant sign/space, stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idKFjzBrY2KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess data and save\n",
        "my_prepro = Prepro(4)\n",
        "SAVE_DIR = r\"data/20news-bydate-train-preprocess\"\n",
        "PATH = r\"data/20news-bydate-train\"\n",
        "\n",
        "# SAVE_DIR = r\"data/20news-bydate-test-preprocess\"\n",
        "# PATH = r\"data/20news-bydate-test\"\n",
        "for d in os.listdir(PATH):\n",
        "  print(d)\n",
        "  try:\n",
        "    os.mkdir(os.path.join(SAVE_DIR, d))\n",
        "  except:\n",
        "    # pass\n",
        "    continue\n",
        "  directory = os.path.join(PATH, d)\n",
        "  for file in os.listdir(directory):\n",
        "    try:\n",
        "      # print(file)\n",
        "      my_file = os.path.join(directory, file)\n",
        "      data = open(my_file, \"r\").read()\n",
        "      text = my_prepro.remove_stop_words(data)\n",
        "      list_words = my_prepro.get_words(text)\n",
        "      word_stem = [my_prepro.stem(w) for w in list_words]\n",
        "      result = \" \".join(word_stem)\n",
        "      save_file = os.path.join(SAVE_DIR, d, file)\n",
        "      with open(save_file, \"w\") as save:\n",
        "        save.write(result)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Kgyevnphuj",
        "colab_type": "text"
      },
      "source": [
        "1. create word freq dict from dir => for remove unusual word\n",
        "2. create word-doc freq for tfidf\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7h8xolMqQlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_prepro = Prepro(4)\n",
        "data_test = my_prepro.get_data_from_dir(r'/content/20news_goup_data/data/20news-bydate-test-preprocess')\n",
        "data_train = my_prepro.get_data_from_dir(r'/content/20news_goup_data/data/20news-bydate-train-preprocess')\n",
        "# all_data = data_test + data_train\n",
        "word_freq, word_doc_freq = my_prepro.create_word_freq(data_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEJK_-bMrqHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = sorted(word_freq.items(), key=lambda x: x[1])\n",
        "word_doc_freq = sorted(word_doc_freq.items(), key=lambda x: x[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTaD2yMMDqhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3c4d9746-652a-41e7-da32-8467050dcea6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 0.6968944099378882),\n",
              " ('organ', 0.9627329192546584),\n",
              " ('line', 0.9971606033717835),\n",
              " ('subject', 1.0),\n",
              " ('from', 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPYii1bBrqKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "156091f6-06d1-450c-d462-ed38f4be5c3c"
      },
      "source": [
        "joblib.dump(word_freq, \"word_freq_train\") # list\n",
        "joblib.dump(word_doc_freq, \"word_doc_freq_train\") #list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['word_doc_freq_train']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0DT39uG62Oa",
        "colab_type": "text"
      },
      "source": [
        "# CounterVecorize, TfIdfVectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOlxwhzBbuVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e42c7208-f20e-4e08-ef19-bebb8d87fd2a"
      },
      "source": [
        "%cd /content/20news_goup_data/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/20news_goup_data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1P1FnAxrqOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54a1c11e-8048-4f46-d077-3265f1893ec1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-bydate-test\t       20news-bydate-train-preprocess  word_freq_train\n",
            "20news-bydate-test-preprocess  README.MD\n",
            "20news-bydate-train\t       word_doc_freq_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCHQfWdirqRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CounterVectorize():\n",
        "  def __init__(self, path_word_freq, df):\n",
        "    self.word_freq = joblib.load(path_word_freq)\n",
        "    self.df = df # for using word that freq > df\n",
        "    all_words = list(filter(lambda x: x[1]>df, self.word_freq))\n",
        "    self.all_words = list(dict(all_words).keys())\n",
        "  def vectorize(self, sentence):\n",
        "    words = sentence.split()\n",
        "    n = len(self.all_words) + 1\n",
        "    vector = [0] * n\n",
        "    for w in words:\n",
        "      flag = False\n",
        "      for i, w2 in enumerate(self.all_words):\n",
        "        if w == w2:\n",
        "          vector[i] += 1\n",
        "          flag = True\n",
        "      if not flag:\n",
        "        vector[n-1] += 1 # word not exist in all_words\n",
        "    return vector\n",
        "  def count_vectorize(self, list_text):\n",
        "    rs = []\n",
        "    for s in list_text:\n",
        "      rs.append(vectorize(s))\n",
        "    return rs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMcDUEcj9Qmr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0feb5a3b-0367-4f4a-8157-56fcd3fbdaeb"
      },
      "source": [
        "c = CounterVectorize('word_freq_train', 3)\n",
        "print(c.vectorize('dog eat meat'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIoo0Tj0_j_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TfIdfVectorize():\n",
        "  def __init__(self, path_word_doc_freq, path_word_freq, df):\n",
        "    self.df = df\n",
        "    word_doc_freq = joblib.load(path_word_doc_freq)\n",
        "    self.word_doc_freq = dict(word_doc_freq)\n",
        "\n",
        "    self.word_freq = joblib.load(path_word_freq)\n",
        "    all_words = list(filter(lambda x: x[1]>df, self.word_freq))\n",
        "    self.all_words = list(dict(all_words).keys())\n",
        "  def vectorize(self, sentence):\n",
        "    words = sentence.split()\n",
        "    n = len(self.all_words) + 1\n",
        "    vector = [0] * n\n",
        "    def tf(words):\n",
        "      n = len(words)\n",
        "      w_freq = Counter(words) # vectorize here is ok\n",
        "      for k in w_freq.keys():\n",
        "        w_freq[k] /= n\n",
        "      return w_freq \n",
        "    def idf(words):\n",
        "      import math\n",
        "      n = len(words)\n",
        "      rs = defaultdict()\n",
        "      for w in words:\n",
        "        r = self.word_doc_freq.get(w, 1)\n",
        "        rs[w] = math.log(1/r) # inverse doc freq\n",
        "      return rs\n",
        "\n",
        "    tf_ = tf(words)\n",
        "    idf_ = idf(words)\n",
        "    for w in words:\n",
        "      tf_idf = tf_[w] * idf_[w]\n",
        "      flag = False\n",
        "      for i, w2 in enumerate(self.all_words):\n",
        "        if w == w2:\n",
        "          vector[i] += tf_idf\n",
        "          flag = True\n",
        "      if not flag:\n",
        "        vector[n-1] += 1 # word not exist in all_words\n",
        "    return vector\n",
        "\n",
        "  def tfidf_vectorize(self, list_text):\n",
        "    rs = []\n",
        "    for sentence in list_text:\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC-QoHvwAhsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = TfIdfVectorize(\"word_doc_freq_train\", \"word_freq_train\", 3)\n",
        "v = t.vectorize(\"my horse in here\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rINUgTE2HoSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "785a4269-48df-4a28-9cf9-e1efaf80dd75"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25008\n",
            "0.5515567054573037\n",
            "25096\n",
            "0.44467818493441813\n",
            "25190\n",
            "0.12211284073374086\n",
            "25194\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThFLoDEQJWjU",
        "colab_type": "text"
      },
      "source": [
        "# CountVectorize + TFIDF SAVE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FObcVtfJfxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "%cd /content/20news_goup_data/data\n",
        "# PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train-preprocess\"\n",
        "PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-test-preprocess\"\n",
        "class_name = os.listdir(PATH_TRAIN)\n",
        "X = []\n",
        "Y = []\n",
        "prepro = Prepro(3)\n",
        "vectorize = CounterVectorize('word_freq_train', 3)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TRAIN, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y += [c] * size\n",
        "  print(\"class {}: \\n\".format(c))\n",
        "  for d in data:\n",
        "    i += 1\n",
        "    X.append(vectorize.vectorize(d))\n",
        "  print(\"done: {}\".format(i/18888))\n",
        "print(\"done!.........\")\n",
        "print(Y[:10])\n",
        "print(X[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWdHcHVEwl0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unFzAlRrlqYH",
        "colab_type": "text"
      },
      "source": [
        "##### Raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju9CsyorltNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92c2b43c-9c54-473d-bd04-71394517f94f"
      },
      "source": [
        "\n",
        "%cd /content/20news_goup_data/data\n",
        "PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train-preprocess\"\n",
        "PATH_TEST = r\"/content/20news_goup_data/data/20news-bydate-test-preprocess\"\n",
        "\n",
        "class_name = os.listdir(PATH_TRAIN)\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TRAIN, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_train += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_train.append(d)\n",
        "\n",
        "# train data\n",
        "class_name = os.listdir(PATH_TEST)\n",
        "\n",
        "X_test = []\n",
        "Y_test = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TEST, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_test += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_test.append(d)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(Y_train)\n",
        "Y_train = le.transform(Y_train)\n",
        "Y_test = le.transform(Y_test)\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/20news_goup_data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMdDP9m2w7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using origin \n",
        "from gensim.utils import simple_preprocess\n",
        "%cd /content/20news_goup_data/data\n",
        "PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train\"\n",
        "PATH_TEST = r\"/content/20news_goup_data/data/20news-bydate-test\"\n",
        "\n",
        "class_name = os.listdir(PATH_TRAIN)\n",
        "\n",
        "X_train_origin = []\n",
        "Y_train_origin = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TRAIN, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_train_origin += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_train_origin.append(simple_preprocess(d))\n",
        "\n",
        "# train data\n",
        "class_name = os.listdir(PATH_TEST)\n",
        "\n",
        "X_test_origin = []\n",
        "Y_test_origin = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TEST, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_test_origin += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_test_origin.append(simple_preprocess(d))\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(Y_train_origin)\n",
        "Y_train_origin = le.transform(Y_train_origin)\n",
        "Y_test = le.transform(Y_test_origin)\n",
        "\n",
        "X_train_origin, Y_train_origin, X_test_origin, Y_test_origin = np.array(X_train_origin), np.array(Y_train_origin), np.array(X_test_origin), np.array(Y_test_origin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8RafBwZxQCo",
        "colab_type": "text"
      },
      "source": [
        "##### Count vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGPN7fSrypLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41ad94ae-b0d8-4a03-8196-a6b42fcfd1e3"
      },
      "source": [
        "# Count vectorize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "%cd /content/20news_goup_data/data\n",
        "PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train-preprocess\"\n",
        "PATH_TEST = r\"/content/20news_goup_data/data/20news-bydate-test-preprocess\"\n",
        "\n",
        "class_name = os.listdir(PATH_TRAIN)\n",
        "\n",
        "X_train_count = []\n",
        "Y_train_count = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TRAIN, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_train_count += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_train_count.append(d)\n",
        "\n",
        "# train data\n",
        "class_name = os.listdir(PATH_TEST)\n",
        "\n",
        "X_test_count = []\n",
        "Y_test_count = []\n",
        "prepro = Prepro(5)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TEST, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_test_count += [c] * size\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_test_count.append(d)\n",
        "\n",
        "vectorize = CountVectorizer(min_df=5)\n",
        "vectorize.fit(X_train_count)\n",
        "X_train_count = vectorize.transform(X_train_count)\n",
        "X_test_count = vectorize.transform(X_test_count)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(Y_train_count)\n",
        "Y_train_transform = le.transform(Y_train_count)\n",
        "Y_test_transform = le.transform(Y_test_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/20news_goup_data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NQqgZO63Cvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_count_idx = [] \n",
        "for i in range(X_train_count.shape[0]):\n",
        "    X_train_count_idx.append(list(np.where(X_train_count[i].toarray() == 1)[1])) # 2d input, not 1d\n",
        "\n",
        "X_test_count_idx = [] \n",
        "for i in range(X_test_count.shape[0]):\n",
        "    X_test_count_idx.append(list(np.where(X_test_count[i].toarray() == 1)[1])) # 2d input, not 1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gd_k8c3q_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "263c2c51-3bc2-4933-8611-d4cd0f598367"
      },
      "source": [
        "len(X_train_count_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11270"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX938bPP2eEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50a890cd-9b84-4f59-f4d2-98c3cadc4838"
      },
      "source": [
        "X_train_count_idx = np.array(X_train_count_idx)\n",
        "X_train_count_idx.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11270, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nmDiP86uax",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coMRu8B0FQI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bb623e6-c7e5-4bdd-b760-03fe664b0699"
      },
      "source": [
        "# TF IDF vectorize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "%cd /content/20news_goup_data/data\n",
        "PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train-preprocess\"\n",
        "# PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-test-preprocess\"\n",
        "class_name = os.listdir(PATH_TRAIN)\n",
        "\n",
        "X_train_tfidf = []\n",
        "Y_train_tfidf = []\n",
        "prepro = Prepro(3)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TRAIN, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_train_tfidf += [c] * size\n",
        "  # print(\"class {}: \\n\".format(c))\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_train_tfidf.append(d)\n",
        "  # print(\"done: {}\".format(i/18888))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/20news_goup_data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHo7LRydgo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "327d7cb8-5861-402c-abc0-48db8f183e23"
      },
      "source": [
        "# TF IDF vectorize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "%cd /content/20news_goup_data/data\n",
        "# PATH_TRAIN = r\"/content/20news_goup_data/data/20news-bydate-train-preprocess\"\n",
        "PATH_TEST = r\"/content/20news_goup_data/data/20news-bydate-test-preprocess\"\n",
        "class_name = os.listdir(PATH_TEST)\n",
        "\n",
        "X_test_tfidf = []\n",
        "Y_test_tfidf = []\n",
        "prepro = Prepro(3)\n",
        "i = 0\n",
        "for c in class_name:\n",
        "  path =  os.path.join(PATH_TEST, c)\n",
        "  data = prepro.get_data_from_dir(path)\n",
        "  size = len(data)\n",
        "  Y_test_tfidf += [c] * size\n",
        "  # print(\"class {}: \\n\".format(c))\n",
        "  for d in data:\n",
        "    i+= 1\n",
        "    X_test_tfidf.append(d)\n",
        "  # print(\"done: {}\".format(i/18888))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/20news_goup_data/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8OKT-eRHG1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorize_tfidf = TfidfVectorizer(min_df=3)\n",
        "vectorize_tfidf.fit(X_train_tfidf)\n",
        "X_train_tfidf = vectorize_tfidf.transform(X_train_tfidf)\n",
        "X_test_tfidf = vectorize_tfidf.transform(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsA4-llnH7Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(Y_train_tfidf)\n",
        "Y_train_transform = le.transform(Y_train_tfidf)\n",
        "Y_test_transform = le.transform(Y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrdHdRSHzKM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d72436f9-b339-4b83-d1dd-845cbd6d132b"
      },
      "source": [
        "X_train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11270, 24888)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gudJ-mUJvFDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a9dcf48-a129-4bcb-a0b6-44fbadc131da"
      },
      "source": [
        "%cd /content/drive/My Drive\n",
        "# !git status\n",
        "joblib.dump(X_test_tfidf, \"X_tfidf_test.lfs\")\n",
        "joblib.dump(Y_train_tfidf, \"Y_tfidf_test\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Y_tfidf_test']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fADGqT0H3YaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "ae488832-6bc8-4a71-a766-468700f7e224"
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,556 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWfSEzfprjT",
        "colab_type": "text"
      },
      "source": [
        "# Load train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WwKlP9eU0j6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = joblib.load(\"/content/drive/My Drive/X_tfidf_train.lfs\")\n",
        "Y_train= joblib.load(\"/content/drive/My Drive/Y_tfidf_train\")\n",
        "X_train = np.asarray(X_train)\n",
        "Y_train = np.asarray(Y_train)\n",
        "\n",
        "X_test = joblib.load(\"/content/drive/My Drive/X_tfidf_test.lfs\")\n",
        "Y_test= joblib.load(\"/content/drive/My Drive/Y_tfidf_test\")\n",
        "X_test = np.asarray(X_test)\n",
        "Y_test = np.asarray(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EioBCckakQRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(Y_train)\n",
        "Y_train_transform = le.transform(Y_train)\n",
        "Y_test_transform = le.transform(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBS90K0YqGWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=1000)\n",
        "pca.fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# X_train_pca = np.concatenate((np.ones((X_train_pca.shape[0], 1)), X_train_pca), axis = 1)\n",
        "# X_test_pca = np.concatenate((np.ones((X_test_pca.shape[0], 1)), X_test_pca), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOCzD_J_cPbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_topic = set(Y_train)\n",
        "# print(all_topic)\n",
        "mapping = dict()\n",
        "for i, t in enumerate(all_topic):\n",
        "  mapping[t] = i\n",
        "# print(mapping)\n",
        "# Y_transform = []\n",
        "# for c in Y:\n",
        "#   Y_transform.append(mapping[c])\n",
        "Y_train_transform = []\n",
        "for c in Y_train:\n",
        "  Y_train_transform.append(mapping[c])\n",
        "\n",
        "Y_test_transform = []\n",
        "for c in Y_test:\n",
        "  Y_test_transform.append(mapping[c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5Xfm2BxJsgF",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKvXCA6KJx1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuitinomialNB1():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.pc = None\n",
        "    self.pxc = None\n",
        "  def fit(self, X, Y):\n",
        "    \"\"\"X: ndarray countvectorize (row vector), Y: ndarray label\"\"\"\n",
        "    def compute_pc(Y):\n",
        "      n = len(Y)\n",
        "      c = Counter(Y)\n",
        "      for k in c.keys():\n",
        "        c[k] /= n\n",
        "      return c\n",
        "\n",
        "    self.pc = compute_pc(Y)\n",
        "\n",
        "    def compute_pxc(X, Y):\n",
        "      # measure ndarray type\n",
        "      X = np.asarray(X)\n",
        "      Y = np.array(Y)\n",
        "      row, col = X.shape[1], len(self.pc)\n",
        "      pxc = np.zeros((row, col))\n",
        "      for c in range(col):\n",
        "        idx = Y==c\n",
        "        # print(\"idx[:10]: \", idx[:10])\n",
        "        n = sum(idx) # size of class c\n",
        "        x = X[idx]\n",
        "        for i in range(row):\n",
        "          # xi = sum(x[:][i]>0)\n",
        "          xi = sum(x[:, i])\n",
        "          pxic = xi/n\n",
        "          pxc[i][c] = pxic\n",
        "      return pxc\n",
        "    self.pxc = compute_pxc(X, Y)\n",
        "  def predict(self, x): # x: countVectorize\n",
        "    n = len(self.pc)\n",
        "    prob = np.zeros(n)\n",
        "    for i in range(n):\n",
        "      p = self.pc[i]\n",
        "      for j in range(len(x)):\n",
        "        if x[j] > 0:\n",
        "          p *= self.pxc[j][i]\n",
        "      prob[i] = p\n",
        "    return np.argmax(prob)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZmS6cGGaca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bayes = MuitinomialNB1()\n",
        "bayes.fit(X, Y_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ4HoSBNJfEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_predict = []\n",
        "for xi in X:\n",
        "  Y_predict.append(bayes.predict(xi))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAwj4OrWOUlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd3557fd-f164-4912-9b8c-8b57c73ed061"
      },
      "source": [
        "wr = 0\n",
        "for a, b in zip(Y_transform, Y_predict):\n",
        "  if a != b:\n",
        "    wr += 1\n",
        "print(1-wr/len(Y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9468500443655723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvJyDmyoVigR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "# f1_score(Y_transform, Y_predict, average='macro') # 0.9529116599715023\n",
        "confusion_matrix(Y_transform, Y_predict)\n",
        "# print(mapping.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGSE2wj4hxaQ",
        "colab_type": "text"
      },
      "source": [
        "## Test NB using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgmXBOHg3x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using lib\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# model = MultinomialNB()\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gaussian = GaussianNB()\n",
        "gaussian.fit(X, Y_transform)\n",
        "Y_predict = []\n",
        "for xi in X:\n",
        "  Y_predict.append(bayes.predict(xi))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGyuuEmUXQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8c1f9111-8595-4e7d-b3fb-c79d04e5479d"
      },
      "source": [
        "# my_prepro = Prepro(4)\n",
        "# all_data = my_prepro.get_data_from_dir(r\"/content/20Newsgroups-classifier/Train/sci.med\") \n",
        "# all_array_data = []\n",
        "# for sentence in all_data:\n",
        "#   s = simple_preprocess(sentence)\n",
        "#   all_array_data.append(s)\n",
        "# my_dict = my_prepro.create_dict_from_array(all_array_data)\n",
        "# print(my_dict)\n",
        "# joblib.dump(my_dict, \"all_dict\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'path': 3, 'cantaloupe': 3, 'srv': 3, 'cs': 6, 'cmu': 5, 'edu': 34, 'das': 1, 'news': 5, 'harvard': 4, 'ogicse': 1, 'emory': 1, 'gatech': 2, 'pitt': 9, 'geb': 4, 'from': 4, 'gordon': 3, 'banks': 3, 'newsgroups': 3, 'sci': 3, 'med': 5, 'subject': 3, 're': 4, 'migraines': 1, 'message': 3, 'id': 3, 'uucp': 2, 'date': 4, 'apr': 6, 'gmt': 3, 'article': 4, 'references': 3, 'mar': 3, 'ntmtv': 1, 'drand': 3, 'spinner': 3, 'osf': 3, 'org': 3, 'sender': 3, 'reply': 1, 'to': 35, 'organization': 3, 'univ': 1, 'of': 37, 'pittsburgh': 1, 'computer': 1, 'science': 1, 'lines': 3, 'in': 18, 'douglas': 1, 'rand': 1, 'writes': 4, 'so': 4, 'll': 3, 'ask': 1, 'this': 8, 'my': 4, 'neurologist': 1, 'just': 4, 'prescribed': 1, 'cafergot': 2, 'and': 22, 'midrin': 1, 'as': 9, 'some': 8, 'alternatives': 1, 'for': 12, 'me': 2, 'try': 3, 'he': 2, 'stated': 1, 'that': 15, 'the': 48, 'sublingual': 1, 'tablets': 2, 'ergotamine': 1, 'were': 2, 'no': 9, 'longer': 3, 'available': 2, 'any': 10, 'idea': 1, 'why': 2, 'also': 4, 'suggested': 2, 'trying': 2, 'mg': 1, 'ibuprophen': 1, 'found': 1, 'out': 5, 'about': 5, 'sublinguals': 1, 'disappearing': 1, 'too': 3, 'don': 8, 'know': 3, 'perhaps': 1, 'because': 2, 'they': 4, 'weren': 1, 'profitable': 1, 'bad': 1, 'since': 2, 'are': 12, 'sometimes': 2, 'vomited': 1, 'up': 2, 'by': 4, 'migraine': 1, 'patients': 5, 'do': 6, 'good': 4, 'flushed': 1, 'down': 1, 'toilet': 1, 'suspect': 2, 'we': 4, 'be': 8, 'moving': 1, 'those': 2, 'more': 8, 'dhe': 1, 'nasal': 1, 'spray': 1, 'which': 5, 'is': 21, 'far': 1, 'effective': 1, 'jxp': 1, 'skepticism': 1, 'chastity': 1, 'intellect': 1, 'cadre': 1, 'dsl': 1, 'it': 9, 'shameful': 1, 'surrender': 1, 'soon': 1, 'magnesium': 1, 'club': 1, 'cc': 1, 'sei': 1, 'cis': 1, 'ohio': 3, 'state': 3, 'magnus': 1, 'acs': 1, 'usenet': 3, 'ins': 1, 'cwru': 1, 'howland': 2, 'reston': 2, 'ans': 2, 'net': 3, 'darwin': 1, 'sura': 1, 'uvaarpa': 1, 'murdoch': 3, 'galen': 2, 'virginia': 5, 'res': 2, 'robert': 1, 'schmieg': 2, 'quack': 9, 'was': 2, 'candida': 3, 'yeast': 4, 'bloom': 2, 'fact': 2, 'or': 10, 'fiction': 2, 'zxsn': 1, 'acc': 2, 'university': 1, 'ravpeinnah': 1, 'gap': 1, 'caltech': 2, 'sat': 2, 'ken': 1, 'isis': 1, 'cns': 1, 'first': 1, 'thing': 1, 'infections': 1, 'but': 10, 'am': 1, 'scientist': 3, 'would': 3, 'take': 2, 'your': 2, 'statement': 1, 'convincing': 4, 'empirical': 1, 'evidence': 8, 'support': 1, 'existence': 2, 'systemic': 1, 'syndrome': 2, 'tell': 1, 'you': 16, 'anything': 2, 'except': 1, 'an': 6, 'absence': 1, 'data': 3, 'on': 6, 'question': 2, 'burden': 1, 'proof': 1, 'rests': 1, 'upon': 2, 'who': 3, 'claim': 1, 'these': 6, 'claims': 1, 'unsubstantiated': 1, 'hopefully': 1, 'issue': 1, 'with': 8, 'anyone': 1, 'overstating': 1, 'their': 4, 'conclusions': 2, 'based': 2, 'beasties': 1, 'present': 1, 'methods': 1, 'even': 3, 'if': 7, 'there': 4, 'noring': 1, 'fellow': 1, 'oklahoma': 1, 'sorry': 1, 'forgot': 1, 'name': 1, 'have': 10, 'one': 3, 'set': 1, 'anecdotal': 4, 'favor': 1, 'personal': 1, 'experiences': 1, 'namely': 1, 'when': 4, 'people': 4, 'certain': 1, 'conditions': 1, 'given': 1, 'anti': 3, 'fungals': 2, 'many': 3, 'them': 1, 'appear': 1, 'get': 1, 'better': 2, 'gee': 1, 'interesting': 1, 'enlightening': 1, 'anecdotes': 1, 'myself': 1, 'friends': 1, 'family': 1, 'practice': 1, 'medicine': 3, 'expect': 1, 'demand': 1, 'rigorous': 1, 'rationales': 1, 'basing': 1, 'therapy': 1, 'than': 3, 'aunt': 1, 'susie': 1, 'brother': 1, 'law': 1, 'may': 4, 'provide': 2, 'inspiration': 1, 'hypothesis': 3, 'rarely': 2, 'proves': 1, 'positive': 2, 'sense': 1, 'unlike': 1, 'mathematics': 1, 'boolean': 1, 'logic': 1, 'applies': 1, 'directly': 1, 'medical': 3, 'issues': 1, 'exceptions': 1, 'does': 1, 'not': 7, 'usually': 1, 'disprove': 1, 'rather': 2, 'modifies': 1, 'current': 1, 'concepts': 1, 'disease': 4, 'against': 1, 'example': 2, 'controlled': 3, 'double': 2, 'blind': 2, 'studies': 1, 'showing': 1, 'sugar': 1, 'water': 1, 'then': 2, 'let': 1, 'hear': 1, 'what': 3, 'uncontrolled': 1, 'side': 2, 'abject': 2, 'disbelief': 2, 'other': 4, 'case': 2, 'please': 1, 'point': 2, 'yelling': 1, 'back': 1, 'forth': 1, 'at': 1, 'each': 2, 'neither': 1, 'has': 4, 'either': 1, 'negative': 1, 'characterize': 1, 'scientific': 1, 'outrage': 1, 'over': 1, 'vastly': 1, 'overstated': 1, 'appears': 1, 'main': 1, 'now': 1, 'whether': 2, 'proponents': 1, 'can': 6, 'marshall': 1, 'enough': 3, 'documented': 1, 'manner': 1, 'make': 2, 'carrying': 1, 'study': 2, 'antifungals': 1, 'else': 2, 'forget': 1, 'anybody': 1, 'carry': 3, 'test': 2, 'themselves': 1, 'adequately': 1, 'define': 1, 'patient': 8, 'population': 1, 'symptoms': 1, 'such': 3, 'should': 2, 'carried': 1, 'fair': 1, 'problem': 2, 'approach': 2, 'happening': 1, 'trenches': 1, 'diagnosis': 2, 'bob': 1, 'rochester': 1, 'udel': 1, 'zaphod': 1, 'mps': 1, 'moe': 1, 'ksu': 2, 'osuunx': 2, 'ucc': 2, 'okstate': 9, 'vms': 7, 'ocom': 7, 'banschbach': 3, 'system': 2, 'nntp': 1, 'posting': 1, 'host': 1, 'osu': 1, 'college': 1, 'osteopathic': 1, 'hsdndev': 2, 'rind': 3, 'enterprise': 1, 'bih': 1, 'david': 5, 'poster': 1, 'being': 3, 'treated': 1, 'liscenced': 2, 'physician': 8, 'did': 1, 'exist': 2, 'calling': 1, 'reprehensible': 1, 'steve': 1, 'see': 2, 'others': 3, 'doing': 2, 'here': 1, 'well': 1, 'believe': 1, 'quacks': 2, 'how': 1, 'diagnoses': 1, 'licensed': 3, 'guarantee': 1, 'someone': 2, 'shouldn': 1, 'say': 1, 'give': 2, 'commonly': 1, 'diagnosed': 1, 'ailment': 1, 'think': 2, 'gotten': 1, 'civilization': 1, 'where': 1, 'need': 3, 'worry': 1, 'unscrupulous': 1, 'healers': 3, 'taking': 1, 'advantage': 1, 'like': 1, 'term': 2, 'applied': 1, 'questionable': 1, 'conduct': 1, 'appropriately': 1, 'called': 1, 'unethical': 2, 'opinion': 1, 'examples': 1, 'prescribing': 2, 'substances': 1, 'demonstrated': 1, 'drug': 1, 'addition': 1, 'medication': 1, 'thyroid': 2, 'preps': 1, 'normal': 1, 'function': 1, 'purpose': 1, 'quick': 1, 'weight': 1, 'loss': 1, 'using': 2, 'laetril': 1, 'treat': 1, 'cancer': 1, 'treatment': 2, 'been': 3, 'shown': 1, 'ineffective': 1, 'dangerous': 1, 'cyanide': 1, 'release': 1, 'nci': 1, 'errors': 2, 'commission': 2, 'competently': 1, 'trained': 1, 'physicians': 2, 'committ': 1, 'omission': 2, 'result': 1, 'malpractice': 1, 'suits': 1, 'fungal': 2, 'agents': 1, 'relieve': 2, 'discomfort': 1, 'having': 1, 'another': 1, 'growth': 1, 'error': 1, 'had': 1, 'long': 2, 'history': 1, 'human': 1, 'suffering': 1, 'stuck': 1, 'standard': 1, 'approved': 1, 'procedures': 1, 'willing': 1, 'reasonable': 1, 'chance': 1, 'will': 2, 'help': 2, 'key': 1, 'tied': 1, 'healer': 1, 'oath': 1, 'harm': 1, 'very': 3, 'few': 2, 'treatments': 1, 'involve': 1, 'risk': 4, 'job': 2, 'difficult': 1, 'versus': 1, 'benefit': 2, 'weighed': 1, 'deals': 1, 'paradox': 1, 'little': 1, 'differently': 1, 'conservative': 1, 'while': 2, 'agressive': 2, 'costly': 1, 'motive': 1, 'improving': 1, 'health': 2, 'attempt': 1, 'rake': 1, 'lots': 1, 'money': 2, 'through': 1, 'schemes': 1, 'uncovered': 1, 'medicare': 1, 'fraud': 1, 'cases': 1, 'label': 1, 'reserve': 1, 'pseudo': 2, 'professionals': 1, 'lurk': 1, 'fringes': 1, 'care': 3, 'waiting': 1, 'frustrated': 1, 'fall': 1, 'into': 1, 'lair': 1, 'individuals': 1, 'really': 2, 'pretty': 1, 'providing': 1, 'alternative': 2, 'lack': 1, 'formal': 1, 'training': 1, 'business': 1, 'simply': 1, 'fast': 1, 'bucks': 1, 'reasonably': 1, 'assured': 1, 'getting': 1, 'competent': 1, 'consulted': 1, 'area': 1, 'buyer': 1, 'beware': 1, 'arena': 1, 'lucky': 1, 'find': 1, 'unlucky': 1, 'loose': 1, 'lot': 1, 'develop': 1, 'severe': 1, 'inability': 1, 'professional': 1, 'diagnose': 1, 'fortay': 1, 'liscened': 1, 'hope': 1, 'clears': 1, 'things': 1, 'marty': 1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['all_dict']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iNwEu5mNCTK",
        "colab_type": "text"
      },
      "source": [
        "#### tfidf load and transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVrsYssYLwbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test tfidf lib\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "X_tfidf_train = joblib.load(\"/content/drive/My Drive/X_tfidf_train.lfs\")\n",
        "Y_tfidf_train = joblib.load(\"/content/drive/My Drive/Y_tfidf_train\")\n",
        "le.fit(Y_tfidf_train)\n",
        "Y_tfidf_train = le.transform(Y_tfidf_train)\n",
        "\n",
        "X_tfidf_test = joblib.load(\"/content/drive/My Drive/X_tfidf_test.lfs\")\n",
        "Y_tfidf_test = joblib.load(\"/content/drive/My Drive/Y_tfidf_test\")\n",
        "Y_tfidf_test = le.transform(Y_tfidf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQSgXpxsNRCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_tfidf_train, Y_tfidf_train)\n",
        "my_y_tfidf_predict = model.predict(X_tfidf_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bimnNKoONthl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "355177c2-077b-47b0-9518-237d713c2bd4"
      },
      "source": [
        "# accuracy_score(my_y_tfidf_predict, Y_tfidf_train)\n",
        "my_y_tfidf_predict_test = model.predict(X_tfidf_test)\n",
        "accuracy_score(my_y_tfidf_predict_test, Y_tfidf_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7964814074370252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xhpE8nigHiK",
        "colab_type": "text"
      },
      "source": [
        "# logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bJaIUu_g9w",
        "colab_type": "text"
      },
      "source": [
        "## softmax regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd1iVERxgJbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "# n: number of feature, m: number of data train\n",
        "# c: number of class\n",
        "# m: number of train data\n",
        "# X(n+1, m): data train column matrix, (append row [1,1,1...] for b0)\n",
        "# W(n+1, c) : weight\n",
        "# Y(c, m): labels matrix, each column is one hot vector, sf labels for 1 observation\n",
        "# Y_predict(c, m): predict labels, shape = shape(Y), each column is probality vector \n",
        "# ()\n",
        "\n",
        "class SoftmaxRegression():\n",
        "  def __init__(self, lr=0.05, tol=1e-4, max_iter=1000, print_after=100):\n",
        "    self.lr = lr # learning rate\n",
        "    self.tol = tol # tolerance\n",
        "    self.max_iter = max_iter # max iteration\n",
        "    self.print_after = print_after\n",
        "    self.w = None\n",
        "  \n",
        "  def linear(self, W, X):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return np.dot(W.T, X) # Z\n",
        "  \n",
        "  def sigmoid(self, Z):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return np.exp(Z) # A\n",
        "  \n",
        "  def softmax(self, Z):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    A = self.sigmoid(Z)\n",
        "    return A / np.sum(A, axis=0)\n",
        "  \n",
        "  def softmax_new(self, Z):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    Z_min = np.max(Z, axis=0, keepdims=True)\n",
        "    Z_ = Z - Z_min\n",
        "    A = self.sigmoid(Z_)\n",
        "    return A / np.sum(A, axis=0)\n",
        "  \n",
        "  def cross_entropy(self, Y_predict, Y):\n",
        "  # \"\"\"return (1, m)\"\"\"\n",
        "    return np.multiply(Y, np.log(Y_predict)) # * for elememtwise is ok too\n",
        "\n",
        "  def cost(self, W, X, Y):\n",
        "    \"\"\" return scalar\"\"\"\n",
        "    Z = self.linear(W, X)\n",
        "    Y_predict = self.softmax_new(Z)\n",
        "    return -np.sum(self.cross_entropy(Y_predict, Y))\n",
        "\n",
        "  def grad(self, W, X, Y):\n",
        "    \"\"\"dw return (n, c)\"\"\"\n",
        "    Z = self.linear(W, X)\n",
        "    # print(\"z: \", Z)\n",
        "    Y_predict = self.softmax_new(Z)\n",
        "    # print(\"predict: \", Y_predict)\n",
        "    return np.dot(X, (Y_predict - Y).T)\n",
        "\n",
        "  def fit(self, X, Y, w_init=[]):\n",
        "    \"\"\"X, Y: ndarray\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    c = Y.shape[0] # number of class\n",
        "    m = Y.shape[1] # no of observation\n",
        "    n = X.shape[0] # number of feature\n",
        "    idx = np.array([i for i in range(m)])\n",
        "    if len(w_init) == 0:\n",
        "      w_init = np.random.randn(X.shape[0], Y.shape[0])\n",
        "    w = w_init\n",
        "    loop = 0\n",
        "    print_after = self.print_after\n",
        "    while loop < self.max_iter:\n",
        "      loop += 1\n",
        "      np.random.shuffle(idx)\n",
        "      for i in idx:\n",
        "        x = X[:, i].reshape(n, 1)\n",
        "        y = Y[:, i].reshape(c, 1)\n",
        "        w1 = deepcopy(w)\n",
        "        w -= self.lr * self.grad(w1, x, y) \n",
        "        if loop % print_after == 0:\n",
        "          # print(\"cost: \", self.cost(w1, X, Y))\n",
        "          if np.linalg.norm(w1-w) < self.tol:\n",
        "            self.w = w\n",
        "            return\n",
        "    self.w = w\n",
        "# batch, epochs, l2\n",
        "  def predict(self, X):\n",
        "    \"\"\"X:column matrix\"\"\"\n",
        "    X = np.asarray(X)\n",
        "    Y_predict = []\n",
        "    for x in X.T:\n",
        "      # x = np.asarray(x).reshape(-1,1)\n",
        "      z = self.linear(self.w, x)\n",
        "      y = self.softmax_new(z)\n",
        "      Y_predict.append(np.argmax(y))\n",
        "    return Y_predict\n",
        "  def predict_origin(self, x):\n",
        "    z = self.linear(self.w, x)\n",
        "    y = self.softmax_new(z)\n",
        "    return np.argmax(y)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCbDl3fREzBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import sparse\n",
        "def convert_labels(y, C):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y), \n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y \n",
        "\n",
        "Y_tfidf_train_soft_max = convert_labels(Y_tfidf_train, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14nGufqAEzKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35f39e79-1112-4bce-f7d2-3a382dcecb8e"
      },
      "source": [
        "model = SoftmaxRegression(max_iter=100, print_after=10)\n",
        "# w = np.random.randn(m, c)\n",
        "X_tfidf_train_soft_max = np.array(X_tfidf_train.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5VK4oj7Fuqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_tfidf_train_soft_max.T, Y_tfidf_train_soft_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dnAAVwTiqqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_tfidf_train_soft_max_predict = model.predict(X_tfidf_train_soft_max.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUUWfWfVm0fJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49579c82-812a-47d5-a8d9-b82249d3b31a"
      },
      "source": [
        "accuracy_score(y_tfidf_train_soft_max_predict, Y_tfidf_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.773114463176575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXHUt36pnwUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y2 = []\n",
        "for c in Y:\n",
        "  Y2.append(mapping[c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULgdzwSin_s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for a, b in zip(tst, Y2):\n",
        "  if a== b:\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lKE5efnoxV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01cc8341-18ea-4646-e797-76c035890871"
      },
      "source": [
        "count/len(Y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5059449866903283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr6t1VbH_p1O",
        "colab_type": "text"
      },
      "source": [
        "## logistic regression for multi label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozhTq1FYrywB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "class LogisticRegression1():\n",
        "  \n",
        "  def __init__(self, lr=0.05, lambd=1e-4, epochs=500):\n",
        "    self.lr = lr\n",
        "    self.lambd = lambd \n",
        "    self.epochs=epochs\n",
        "    self.w = []\n",
        "\n",
        "  def linear(self, W, X):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return np.dot(W.T, X) # Z\n",
        "  \n",
        "  def sigmoid(self, Z):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return 1/ (1+np.exp(-Z)) # A\n",
        "  \n",
        "  def grad(self, W, X, Y):\n",
        "    Z = self.linear(W, X)\n",
        "    A = self.sigmoid(Z)\n",
        "    # return np.multiply((A-Y), X)\n",
        "    return np.multiply((A-Y), X) + self.lambd * W\n",
        "\n",
        "  \n",
        "  def fit(self, X, Y, w_init=[]):\n",
        "    d = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    if len(w_init) == 0:\n",
        "      w_init = np.random.randn(d, 1)\n",
        "    w0 = w_init\n",
        "    all_idx = np.array([i for i in range(m)])\n",
        "    loop = 0\n",
        "    max_loop = self.epochs\n",
        "    while(loop<max_loop):\n",
        "      # start = time()\n",
        "      all_idx = np.random.permutation(all_idx)\n",
        "      for i in all_idx:\n",
        "        xi = X[:, i].reshape(d, 1)\n",
        "        yi = Y[i]\n",
        "        # w_new = w0.reshape(d, 1) - self.lr * self.grad(w0, xi, yi).reshape(d, 1)\n",
        "        # t0_grad = time()\n",
        "        w_new = w0 - self.lr * self.grad(w0, xi, yi)\n",
        "        # t1_grad = time()\n",
        "        # print(f\"compute grad after{t1_grad-t0_grad}\")\n",
        "        w0 = copy.deepcopy(w_new)\n",
        "      end = time()\n",
        "      # print(f\"done loop {loop} after {end-start}s\")\n",
        "      loop += 1\n",
        "    # self.w = w0\n",
        "    return w0\n",
        "  \n",
        "  def fit_all(self, X, Y, w_init=[]):\n",
        "    \"\"\"\n",
        "    Y: label, range(0, c)\n",
        "    X, W: colmn matrix\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    classes = set(Y) # range(1, c)\n",
        "    c = len(classes) # nof class\n",
        "    d = X.shape[0] # dimension\n",
        "    m = X.shape[1] # nof train data\n",
        "\n",
        "    if len(w_init) == 0:\n",
        "      w_init = np.random.randn(d, c)\n",
        "    \n",
        "    w = np.array([])\n",
        "    # 1 vs rest.\n",
        "    for idx in range(c):\n",
        "      t1 = time()\n",
        "      Y_train_tmp = Y == idx\n",
        "      wc = self.fit(X, Y_train_tmp, w_init[:, idx].reshape(d, 1))\n",
        "      # print(\"shape wc: \", wc.shape)\n",
        "      w = np.append(w, [wc])\n",
        "      # print(f\"w[{idx}]: \", wc)\n",
        "      t2 = time()\n",
        "      print(f\"done class {idx} after {t2-t1}s!\\n\")\n",
        "    self.w = w.reshape(d,c, order='F')\n",
        "\n",
        "  # def predict(self, X):\n",
        "  #   Z = self.linear(self.w, X)\n",
        "  #   A = self.sigmoid(Z)\n",
        "  #   return A\n",
        "\n",
        "  def predict(self, x):\n",
        "    Z = np.dot(self.w.T, x)\n",
        "    A = self.sigmoid(Z)\n",
        "    # print(A)\n",
        "    return np.argmax(A, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0g7PvWQy4pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "class LogisticRegression1():\n",
        "  \n",
        "  def __init__(self, lr=0.05, lambd=1e-5, batch=64, epochs=500):\n",
        "    self.lr = lr\n",
        "    self.lambd = lambd \n",
        "    self.batch = batch\n",
        "    self.epochs = epochs\n",
        "    self.w = []\n",
        "\n",
        "  def linear(self, W, X):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return np.dot(W.T, X) # Z\n",
        "  \n",
        "  def sigmoid(self, Z):\n",
        "    \"\"\"return (c, m)\"\"\"\n",
        "    return 1/ (1+np.exp(-Z)) # A\n",
        "  \n",
        "  def grad(self, W, X, Y):\n",
        "    Z = self.linear(W, X)\n",
        "    A = self.sigmoid(Z)\n",
        "    # return (A-Y) * X\n",
        "    return np.average(np.multiply((A-Y), X), axis=1) + self.lambd * W\n",
        "\n",
        "  \n",
        "  def fit(self, X, Y, w_init=[]):\n",
        "    d = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    if len(w_init) == 0:\n",
        "      w_init = np.random.randn(d, 1)\n",
        "    w0 = w_init\n",
        "    all_idx = np.array([i for i in range(m)])\n",
        "    loop = 0\n",
        "    max_loop = self.epochs\n",
        "\n",
        "    k = m // self.batch\n",
        "\n",
        "    while(loop<max_loop):\n",
        "      # start = time()\n",
        "      all_idx = np.random.permutation(all_idx)\n",
        "      for i in range(k):\n",
        "        current_idx = all_idx[self.batch*i:self.batch*(i+1)]\n",
        "        xi = X[:, current_idx] #.reshape(d, 1)\n",
        "        yi = Y[current_idx]\n",
        "        # w_new = w0.reshape(d, 1) - self.lr * self.grad(w0, xi, yi).reshape(d, 1)\n",
        "        # t0_grad = time()\n",
        "        w_new = w0 - self.lr * self.grad(w0, xi, yi)\n",
        "        # t1_grad = time()\n",
        "        # print(f\"compute grad after{t1_grad-t0_grad}\")\n",
        "        w0 = copy.deepcopy(w_new)\n",
        "      # end = time()\n",
        "      # print(f\"done loop {loop} after {end-start}s\")\n",
        "      loop += 1\n",
        "    # self.w = w0\n",
        "    return w0\n",
        "  \n",
        "  def fit_all(self, X, Y, w_init=[]):\n",
        "    \"\"\"\n",
        "    Y: label, range(0, c)\n",
        "    X, W: colmn matrix\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    classes = set(Y) # range(1, c)\n",
        "    c = len(classes) # nof class\n",
        "    d = X.shape[0] # dimension\n",
        "    m = X.shape[1] # nof train data\n",
        "\n",
        "    if len(w_init) == 0:\n",
        "      w_init = np.random.randn(d, c)\n",
        "    \n",
        "    w = np.array([])\n",
        "    # 1 vs rest.\n",
        "    for idx in range(c):\n",
        "      t1 = time()\n",
        "      Y_train_tmp = Y == idx\n",
        "      wc = self.fit(X, Y_train_tmp, w_init[:, idx].reshape(d, 1))\n",
        "      # print(\"shape wc: \", wc.shape)\n",
        "      w = np.append(w, [wc])\n",
        "      # print(f\"w[{idx}]: \", wc)\n",
        "      t2 = time()\n",
        "      print(f\"done class {idx} after {t2-t1}s!\\n\")\n",
        "    self.w = w.reshape(d,c, order='F')\n",
        "\n",
        "  # def predict(self, X):\n",
        "  #   Z = self.linear(self.w, X)\n",
        "  #   A = self.sigmoid(Z)\n",
        "  #   return A\n",
        "\n",
        "  def predict(self, x):\n",
        "    Z = np.dot(self.w.T, x)\n",
        "    A = self.sigmoid(Z)\n",
        "    # print(A)\n",
        "    return np.argmax(A, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk6-ACIRc1hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "b6893a9d-b440-4faa-c62e-440aec837740"
      },
      "source": [
        "from time import time\n",
        "t1 = time()\n",
        "my_model = LogisticRegression1()\n",
        "my_model.fit_all(X_train_pca.T, Y_train_transform)\n",
        "my_y_train_predict = my_model.predict(X_train_pca.T)\n",
        "my_y_test_predict = my_model.predict(X_test_pca.T)\n",
        "t2 = time()\n",
        "print(f\"train and predict done after: {t2-t1}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done class 0 after 141.02648043632507s!\n",
            "\n",
            "done class 1 after 140.82420539855957s!\n",
            "\n",
            "done class 2 after 139.83836245536804s!\n",
            "\n",
            "done class 3 after 141.41368412971497s!\n",
            "\n",
            "done class 4 after 141.84718585014343s!\n",
            "\n",
            "done class 5 after 141.0386447906494s!\n",
            "\n",
            "done class 6 after 142.31522798538208s!\n",
            "\n",
            "done class 7 after 140.42853379249573s!\n",
            "\n",
            "done class 8 after 140.2455861568451s!\n",
            "\n",
            "done class 9 after 141.56627535820007s!\n",
            "\n",
            "done class 10 after 141.48139810562134s!\n",
            "\n",
            "done class 11 after 141.45578050613403s!\n",
            "\n",
            "done class 12 after 141.18538522720337s!\n",
            "\n",
            "done class 13 after 142.58256077766418s!\n",
            "\n",
            "done class 14 after 140.83732151985168s!\n",
            "\n",
            "done class 15 after 141.08490633964539s!\n",
            "\n",
            "done class 16 after 140.5986840724945s!\n",
            "\n",
            "done class 17 after 141.15088891983032s!\n",
            "\n",
            "done class 18 after 142.03147506713867s!\n",
            "\n",
            "done class 19 after 141.83304715156555s!\n",
            "\n",
            "train and predict done after: 2825.0558116436005s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ5lw8XGvVK7",
        "colab_type": "text"
      },
      "source": [
        "without regularization, SGD:\n",
        "- training time: 488 s\n",
        "- mymodel:\n",
        "  - train accuracy: 0.9740017746228926\n",
        "  - test accuracy: 0.7083833133413301\n",
        "- sklearn model: \n",
        "  - train accuracy: 0.9940023990403839\n",
        "  - test accuracy: 0.711182193789151"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeiKnrSzvR9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81c7ed70-e8f9-48f0-da00-fc7eec863e2c"
      },
      "source": [
        "print(accuracy_score(my_y_train_predict, Y_train_transform))\n",
        "print(accuracy_score(my_y_test_predict, Y_test_transform))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8695652173913043\n",
            "0.6783953085432494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snuz9eQfc5qU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ff692c40-9345-42bc-93b2-215de47441f9"
      },
      "source": [
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([[0, 2, 3, 4], [1, 2, 3, 4]])\n",
        "print(a)\n",
        "print(b)\n",
        "print(a*b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n",
            "[[0 2 3 4]\n",
            " [1 2 3 4]]\n",
            "[[ 0  4  9 16]\n",
            " [ 1  4  9 16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DLPt68VUUno",
        "colab_type": "text"
      },
      "source": [
        "### test using sklearn and tf-idf vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRBQFvot6-R-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c84e5e2f-eedb-4d18-d016-ae9246f00b74"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lr_sklearn = LogisticRegression(C=10, max_iter=500)\n",
        "model_lr_sklearn.fit(X_tfidf_train, Y_tfidf_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGB9NnyONxYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c96f689-5c38-4b60-f2e9-eda642bfe69e"
      },
      "source": [
        "my_y_train_tfidf_predict = model_lr_sklearn.predict(X_tfidf_train)\n",
        "print(accuracy_score(my_y_train_tfidf_predict, Y_tfidf_train))\n",
        "\n",
        "my_y_test_tfidf_predict = model_lr_sklearn.predict(X_tfidf_test)\n",
        "print(accuracy_score(my_y_test_tfidf_predict, Y_tfidf_test))\n",
        "\n",
        "# c=1: \n",
        "# 0.9740017746228926\n",
        "# 0.8300679728108756\n",
        "# c=2\n",
        "# max_iter=200\n",
        "# 0.9943212067435669\n",
        "# 0.8412634946021591\n",
        "# c=10\n",
        "# 0.9991126885536823\n",
        "# 0.8423297347727575"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9991126885536823\n",
            "0.8423297347727575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix9Bvm1sUaQO",
        "colab_type": "text"
      },
      "source": [
        "#### using pca:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwwBK1IYPFBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_tfidf = TruncatedSVD(n_components=2000)\n",
        "svd_tfidf.fit(X_tfidf_train)\n",
        "X_train_pca = svd_tfidf.transform(X_tfidf_train)\n",
        "X_test_pca = svd_tfidf.transform(X_tfidf_test)\n",
        "\n",
        "X_tfidf_train_pca = np.concatenate((np.ones((X_train_pca.shape[0], 1)), X_train_pca), axis = 1)\n",
        "X_tfidf_test_pca = np.concatenate((np.ones((X_test_pca.shape[0], 1)), X_test_pca), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSGxUvPxVSnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cbc511bc-2b6a-46d7-bcd1-81e54cb2b01a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lr_sklearn_svd = LogisticRegression(C=10, max_iter=500)\n",
        "model_lr_sklearn_svd.fit(X_tfidf_train_pca, Y_tfidf_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzm_H2uY0ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "152bf58a-3f5d-4ea4-e224-6378518a70ee"
      },
      "source": [
        "my_y_train_tfidf_predict = model_lr_sklearn_svd.predict(X_tfidf_train_pca)\n",
        "print(accuracy_score(my_y_train_tfidf_predict, Y_tfidf_train))\n",
        "\n",
        "my_y_test_tfidf_predict = model_lr_sklearn_svd.predict(X_tfidf_test_pca)\n",
        "print(accuracy_score(my_y_test_tfidf_predict, Y_tfidf_test))\n",
        "# c=1\n",
        "# 0.9464063886424134\n",
        "# 0.8170065307210449\n",
        "# c=0.5\n",
        "# 0.9224489795918367\n",
        "# 0.8087431693989071\n",
        "# C=5\n",
        "# 0.9827861579414374\n",
        "# 0.83046781287485\n",
        "# c=10\n",
        "# 0.9917480035492457\n",
        "# 0.831134212981474"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9917480035492457\n",
            "0.831134212981474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvTpDk4oZ1hi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "14b63ece-e4b3-424c-8d2b-344633f1fd5e"
      },
      "source": [
        "# my model\n",
        "from time import time\n",
        "t1 = time()\n",
        "my_model = LogisticRegression1(lambd=0)\n",
        "my_model.fit_all(X_tfidf_train_pca.T, Y_tfidf_train)\n",
        "my_y_train_predict = my_model.predict(X_tfidf_train_pca.T)\n",
        "my_y_test_predict = my_model.predict(X_tfidf_test_pca.T)\n",
        "t2 = time()\n",
        "print(f\"train and predict done after: {t2-t1}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done class 0 after 159.36784434318542s!\n",
            "\n",
            "done class 1 after 160.41769528388977s!\n",
            "\n",
            "done class 2 after 159.56728410720825s!\n",
            "\n",
            "done class 3 after 159.61383414268494s!\n",
            "\n",
            "done class 4 after 159.84790682792664s!\n",
            "\n",
            "done class 5 after 159.8946452140808s!\n",
            "\n",
            "done class 6 after 160.66920280456543s!\n",
            "\n",
            "done class 7 after 159.25117993354797s!\n",
            "\n",
            "done class 8 after 158.53119707107544s!\n",
            "\n",
            "done class 9 after 158.91223001480103s!\n",
            "\n",
            "done class 10 after 159.04833436012268s!\n",
            "\n",
            "done class 11 after 159.09519791603088s!\n",
            "\n",
            "done class 12 after 160.72660875320435s!\n",
            "\n",
            "done class 13 after 158.76486921310425s!\n",
            "\n",
            "done class 14 after 158.82722568511963s!\n",
            "\n",
            "done class 15 after 159.17305278778076s!\n",
            "\n",
            "done class 16 after 158.65871024131775s!\n",
            "\n",
            "done class 17 after 160.78442478179932s!\n",
            "\n",
            "done class 18 after 158.0179316997528s!\n",
            "\n",
            "done class 19 after 158.43036770820618s!\n",
            "\n",
            "train and predict done after: 3187.751258134842s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxQjvWWjdZlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cd887b65-4963-4b0f-e0c0-e6b33d85afd1"
      },
      "source": [
        "print(accuracy_score(my_y_train_predict, Y_tfidf_train))\n",
        "print(accuracy_score(my_y_test_predict, Y_tfidf_test))\n",
        "# lambd = 1e-4, num epochs = 500 => t=3300\n",
        "# 0.9300798580301686\n",
        "# 0.8075436492069838\n",
        "# lambd = 0\n",
        "# 0.9961845607808341\n",
        "# 0.8290017326402772"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9961845607808341\n",
            "0.8290017326402772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGU_iHiMqXuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGQ4DFe4HPzB",
        "colab_type": "text"
      },
      "source": [
        "# Kmeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1n8FclTHRe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import logging\n",
        "\n",
        "class KMeans1():\n",
        "  # all array use below are ndarray, no comment!\n",
        "  def __init__(self, n_clusters):\n",
        "    self.n_clusters = n_clusters\n",
        "    self.cluster_centers_ = None\n",
        "    self.logger = logging.getLogger()\n",
        "  def fit(self, X):\n",
        "    # X: row data\n",
        "    n, d = X.shape\n",
        "\n",
        "    # random init labels\n",
        "    Y = np.random.randint(0, self.n_clusters, n)\n",
        "\n",
        "    # place holder center\n",
        "    centers = np.zeros((self.n_clusters, d))\n",
        "\n",
        "    count = 0\n",
        "    while count<1000:\n",
        "      self.logger.warning(f\"------------------------\\n\\n\")\n",
        "      self.logger.warning(f\"counter: {Counter(Y)}!\")\n",
        "      count+= 1\n",
        "      # if count % 10 == 0:\n",
        "      #   self.logger.warning(f\"loop {count}!\")\n",
        "      # compute new center:\n",
        "      for i in range(self.n_clusters):\n",
        "        centers[i] = np.mean(X[Y==i], axis=0)\n",
        "      \n",
        "      # assign class\n",
        "      ## compute distance to center\n",
        "      distance_matrix = cdist(X, centers)\n",
        "      ## assign new label\n",
        "      Y_new = np.argmin(distance_matrix, axis=1)\n",
        "      if (Y_new == Y).all():\n",
        "        print(f\"break after {count} iterations\")\n",
        "        break\n",
        "      Y = np.copy(Y_new)\n",
        "\n",
        "    self.cluster_centers_ = centers\n",
        "    # return Y\n",
        "\n",
        "  def predict(self, X):\n",
        "    distance_matrix = cdist(X, self.cluster_centers_)\n",
        "    labels = np.argmin(distance_matrix, axis=1)\n",
        "    return labels\n",
        "\n",
        "  @staticmethod\n",
        "  def score(Y_true, Y_predict):\n",
        "    # assume: max true\n",
        "    # consider input order\n",
        "    n_clusters = len(set(Y_true))\n",
        "    Y_true_transform = np.zeros_like(Y_true)\n",
        "    Y_predict_transform = np.copy(Y_predict)\n",
        "    Y_predict_copy = np.copy(Y_predict)\n",
        "\n",
        "    for i in range(n_clusters):\n",
        "      yi = Y_true==i\n",
        "      yi_pred = Y_predict_copy[yi]\n",
        "      i_map = max(Counter(yi_pred).items(), key=lambda x: x[1] if x[0] >= 0 else -1)[0] # become positive ok!\n",
        "      Y_true_transform[yi] = i_map\n",
        "      Y_predict_copy[Y_predict_copy==i_map] = -1 # mark, change Y_predict here\n",
        "    \n",
        "    sc = accuracy_score(Y_true_transform, Y_predict_transform)\n",
        "    return sc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kymmekSZZsIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=10000)\n",
        "svd.fit(X_train_tfidf)\n",
        "X_train_svd = svd.transform(X_train_tfidf)\n",
        "# X_test_svd = svd.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQQmK3NlJ1Dl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9cff117b-b4e5-4fbb-afde-9bc4bda91374"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from time import time\n",
        "t1 = time()\n",
        "kmeans = KMeans(n_clusters=20, random_state=0).fit(X_train_tfidf)\n",
        "t2 = time()\n",
        "print(f\"training time: {t2-t1}\")\n",
        "print('Centers found by scikit-learn:')\n",
        "print(kmeans.cluster_centers_)\n",
        "pred_label = kmeans.predict(X_train_tfidf)\n",
        "pred_label_test = kmeans.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training time: 575.2834944725037\n",
            "Centers found by scikit-learn:\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [4.00057922e-06 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPWn0EZnJ2fx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "4a2e581d-e1b0-45ae-b1da-d96170b332bf"
      },
      "source": [
        "model = KMeans1(20)\n",
        "t1 = time()\n",
        "model.fit(X_train_tfidf.toarray())\n",
        "t2 = time()\n",
        "print(f\"training after: {t2-t1}s\")\n",
        "lbs = model.predict(X_train_tfidf.toarray())\n",
        "lbs_test = model.predict(X_test_tfidf.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({12: 589, 9: 587, 3: 582, 10: 578, 1: 578, 4: 576, 5: 576, 6: 576, 7: 570, 2: 568, 16: 567, 14: 557, 0: 555, 18: 551, 8: 550, 19: 550, 15: 550, 11: 540, 13: 538, 17: 532})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({6: 674, 9: 644, 0: 625, 3: 606, 2: 599, 7: 596, 12: 584, 5: 576, 15: 571, 19: 562, 11: 560, 18: 543, 16: 526, 8: 522, 10: 521, 1: 519, 14: 519, 4: 514, 13: 507, 17: 502})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({18: 821, 3: 783, 19: 740, 15: 728, 5: 725, 12: 648, 9: 619, 2: 553, 16: 551, 11: 533, 13: 501, 7: 488, 8: 484, 6: 483, 17: 477, 1: 460, 0: 448, 4: 426, 14: 403, 10: 399})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({18: 1001, 15: 970, 19: 897, 3: 853, 5: 771, 16: 612, 12: 580, 14: 539, 9: 515, 2: 513, 10: 510, 13: 510, 17: 497, 11: 418, 8: 416, 7: 403, 4: 376, 0: 328, 6: 290, 1: 271})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1145, 18: 1112, 19: 1034, 3: 780, 14: 766, 5: 746, 16: 659, 10: 636, 12: 538, 2: 480, 17: 461, 13: 455, 9: 429, 7: 377, 8: 367, 4: 343, 11: 303, 0: 248, 6: 206, 1: 185})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1277, 18: 1127, 19: 1099, 14: 1018, 16: 759, 5: 728, 3: 710, 10: 581, 12: 511, 2: 446, 17: 446, 9: 417, 13: 415, 7: 369, 8: 324, 4: 302, 11: 240, 0: 217, 6: 169, 1: 115})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1381, 14: 1239, 18: 1112, 19: 1099, 16: 806, 5: 706, 3: 652, 10: 547, 12: 454, 17: 427, 2: 426, 9: 408, 13: 403, 7: 367, 8: 306, 4: 285, 11: 205, 0: 195, 6: 162, 1: 90})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1489, 14: 1391, 19: 1118, 18: 1060, 16: 812, 5: 692, 3: 626, 10: 527, 17: 422, 13: 401, 2: 400, 9: 399, 12: 397, 7: 359, 8: 293, 4: 282, 0: 184, 11: 180, 6: 158, 1: 80})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1625, 14: 1445, 19: 1127, 18: 1033, 16: 835, 5: 673, 3: 608, 10: 504, 17: 421, 13: 399, 9: 391, 12: 363, 2: 362, 7: 355, 4: 282, 8: 278, 0: 179, 11: 159, 6: 153, 1: 78})!\n",
            "------------------------\n",
            "\n",
            "\n",
            "counter: Counter({15: 1748, 14: 1483, 19: 1146, 18: 1009, 16: 853, 5: 651, 3: 596, 10: 485, 17: 420, 13: 395, 9: 383, 7: 354, 12: 349, 2: 310, 4: 276, 8: 257, 0: 177, 6: 152, 11: 148, 1: 78})!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training after: 180.50415587425232s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w7nbokulZYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_purity(predicted,expected):\n",
        "  majority_sum=0\n",
        "  for cluster_index in range(20):\n",
        "    member_indexs=np.where(predicted==cluster_index)[0]\n",
        "    expected_labels=[expected[index]for index in member_indexs]\n",
        "    max_count=max(expected_labels.count(la)for la in range(20))\n",
        "    majority_sum+=max_count\n",
        "  print(majority_sum)\n",
        "  return majority_sum/len(expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvir6N88hIJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7736d73c-d40c-4145-94dc-c77c29fe9aaa"
      },
      "source": [
        "compute_purity(lbs_test, Y_test_transform)\n",
        "# train: 0.3801242236024845, time: 575.2834944725037\n",
        "# test: 0.20488021295474712\n",
        "\n",
        "# my model:\n",
        "# train: 180.50415587425232s,  0.4200532386867791\n",
        "# test: 0.351192856190857"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.351192856190857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwfpHn9pugbr",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCbsJmWEu8H2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3d7e8dd-b9df-409f-ce8b-205f411c84fb"
      },
      "source": [
        "X_train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11270, 70733)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hcH2tornVm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b293a9a4-e930-49d4-b37e-aa8b06a43f12"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from time import time\n",
        "svc = SVC(kernel='linear', gamma='auto', C=0.1)\n",
        "# C=1.0, kernel='linear', degree=3, gamma='auto'\n",
        "t1 = time()\n",
        "svc.fit(X_train_tfidf, Y_train_transform)\n",
        "t2 = time()\n",
        "print(f\"done after {t2-t1} s\") # rbf: 260s, poly: 300.9100682735443 s, linear: 120.51525282859802 s\n",
        "y_train_pred = svc.predict(X_train_tfidf)\n",
        "y_test_pred = svc.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done after 217.05814266204834 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PvTXbij0Wkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea3a1356-3da9-47f0-e6cc-755d20a89140"
      },
      "source": [
        "print(accuracy_score(y_train_pred, Y_train_transform))\n",
        "print(accuracy_score(y_test_pred, Y_test_transform))\n",
        "# c default: \n",
        "# rbf:\n",
        "# 0.9965394853593611\n",
        "# 0.8260695721711315\n",
        "\n",
        "# poly:\n",
        "# 0.9990239574090506\n",
        "# 0.5913634546181528\n",
        "\n",
        "# c=5, rbf:\n",
        "# 0.9993788819875776\n",
        "# 0.8279354924696788\n",
        "\n",
        "# linear:\n",
        "# 0.9899733806566104\n",
        "# 0.8295348527255765"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.860958296362023\n",
            "0.726109556177529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXI256mtu9bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not using now\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_tfidf = TruncatedSVD(n_components=20000)\n",
        "t1 = time()\n",
        "svd_tfidf.fit(X_train_tfidf)\n",
        "t2 = time()\n",
        "print(f\"{t2-t1}s done!\")\n",
        "X_train_tfidf_svd = svd_tfidf.transform(X_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7AM3m61YLzk",
        "colab_type": "text"
      },
      "source": [
        "# Neural network with pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_AeinShRQcM",
        "colab_type": "text"
      },
      "source": [
        "## Classical neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQQ68VniYK6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucRBIOXbYo58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Softmax, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "    self.activate1 = torch.nn.Sigmoid()\n",
        "    self.activate2 = torch.nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "    out = self.activate1(out)\n",
        "    out = self.activate2(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKGEoQQaRf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3ac33ba-629a-46fc-bb96-236d08ea9e86"
      },
      "source": [
        "X_train_tfidf.shape, Y_train_transform.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11270, 24888), (11270,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqXYWnCzaFbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Softmax(24888, 20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 100\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "  if torch.cuda.is_available():\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float().cuda())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform).cuda())\n",
        "  else:\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform))\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  output = model(X)\n",
        "  loss = criterion(output, Y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"loss after {epoch} epochs: {loss}\")\n",
        "\n",
        "print(f\"train {epoch} epochs after {time()-t1} s\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLkRPt4LcrQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "766cc2ac-169f-4b2b-bf6b-2c5f4b6af309"
      },
      "source": [
        "pred = torch.argmax(model(torch.from_numpy(X_train_tfidf.toarray()).float().cuda()), axis=1)\n",
        "sum(pred == torch.from_numpy(Y_train_transform).cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11091, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPW6Iw-TvnJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e5e2684-040c-4c34-e295-c4a598b02cb0"
      },
      "source": [
        "pred = torch.argmax(model(torch.from_numpy(X_test_tfidf.toarray()).float().cuda()), axis=1)\n",
        "sum(pred == torch.from_numpy(Y_test_transform).cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5968, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XDd7as-dGtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5908e31-564b-44a3-92a3-a4fd33fdf74d"
      },
      "source": [
        "# softmax: traintime: 3s\n",
        "# 11090/11270 = 0.9840283939662822\n",
        "# 5968/7503 = 0.7954151672664268"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9840283939662822"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvEImcnXTOaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "74a73651-636e-4c16-b288-58beb2c4e07d"
      },
      "source": [
        "model = Softmax(24888, 20)\n",
        "summary(model, (49776, 24888))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1            [-1, 49776, 20]         497,780\n",
            "           Sigmoid-2            [-1, 49776, 20]               0\n",
            "           Softmax-3            [-1, 49776, 20]               0\n",
            "================================================================\n",
            "Total params: 497,780\n",
            "Trainable params: 497,780\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4725.74\n",
            "Forward/backward pass size (MB): 22.79\n",
            "Params size (MB): 1.90\n",
            "Estimated Total Size (MB): 4750.43\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G5LDjGNTPvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxR4voOOh67e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxMs(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(SoftmaxMs, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(input_dim, 200)\n",
        "    self.activ1 = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(200, 100)\n",
        "    self.activ2 = torch.nn.ReLU()\n",
        "    self.linear3 = torch.nn.Linear(100, output_dim)\n",
        "    self.activ3 = torch.nn.Sigmoid()\n",
        "\n",
        "    self.activend = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    self.batchnorm2 = torch.nn.BatchNorm1d(200)\n",
        "    self.dropout1 = torch.nn.Dropout(0.4)\n",
        "    self.dropout2 = torch.nn.Dropout(0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.linear1(x)\n",
        "    output = self.activ1(output)\n",
        "    output = self.dropout1(output)\n",
        "\n",
        "    output = self.batchnorm2(output) \n",
        "    output = self.linear2(output)\n",
        "    output = self.activ2(output)\n",
        "    output = self.dropout2(output)\n",
        "\n",
        "    output = self.linear3(output)\n",
        "    output = self.activ3(output)\n",
        "    output = self.activend(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8XptyXCs7_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "230e7383-93da-4bb6-91be-c78183726c24"
      },
      "source": [
        "model = SoftmaxEvol(24888, 20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 31\n",
        "from time import time\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "  if torch.cuda.is_available():\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float().cuda())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform).cuda())\n",
        "  else:\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform))\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  output = model(X)\n",
        "  loss = criterion(output, Y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 2 == 0:\n",
        "    print(f\"loss after {epoch} epochs: {loss}\")\n",
        "    pred1 = torch.argmax(model(torch.from_numpy(X_train_tfidf.toarray()).float().cuda()), axis=1)\n",
        "    s1 = sum(pred1 == torch.from_numpy(Y_train_transform).cuda())\n",
        "    print(f\"true train: {s1}\")\n",
        "\n",
        "    pred2 = torch.argmax(model(torch.from_numpy(X_test_tfidf.toarray()).float().cuda()), axis=1)\n",
        "    s2 = sum(pred2 == torch.from_numpy(Y_test_transform).cuda())\n",
        "    print(f\"true test: {s2}\")\n",
        "\n",
        "print(f\"training {epochs} epochs done after {time()-t1} s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after 0 epochs: 2.9957852363586426\n",
            "true train: 7370\n",
            "true test: 3815\n",
            "loss after 2 epochs: 2.980170726776123\n",
            "true train: 9947\n",
            "true test: 5346\n",
            "loss after 4 epochs: 2.9692044258117676\n",
            "true train: 10439\n",
            "true test: 5630\n",
            "loss after 6 epochs: 2.9583020210266113\n",
            "true train: 10689\n",
            "true test: 5803\n",
            "loss after 8 epochs: 2.9477012157440186\n",
            "true train: 10871\n",
            "true test: 5856\n",
            "loss after 10 epochs: 2.9381468296051025\n",
            "true train: 11010\n",
            "true test: 5959\n",
            "loss after 12 epochs: 2.930889844894409\n",
            "true train: 11096\n",
            "true test: 5988\n",
            "loss after 14 epochs: 2.9269700050354004\n",
            "true train: 11158\n",
            "true test: 6054\n",
            "loss after 16 epochs: 2.9248554706573486\n",
            "true train: 11208\n",
            "true test: 6071\n",
            "loss after 18 epochs: 2.9233345985412598\n",
            "true train: 11235\n",
            "true test: 6091\n",
            "loss after 20 epochs: 2.922386646270752\n",
            "true train: 11245\n",
            "true test: 6113\n",
            "loss after 22 epochs: 2.921751022338867\n",
            "true train: 11254\n",
            "true test: 6136\n",
            "loss after 24 epochs: 2.921435832977295\n",
            "true train: 11255\n",
            "true test: 6097\n",
            "loss after 26 epochs: 2.9212393760681152\n",
            "true train: 11250\n",
            "true test: 6126\n",
            "loss after 28 epochs: 2.9211158752441406\n",
            "true train: 11255\n",
            "true test: 6101\n",
            "loss after 30 epochs: 2.9210472106933594\n",
            "true train: 11260\n",
            "true test: 6102\n",
            "training 31 epochs done after 67.8350510597229 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Is33ofkZODe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4147d7f3-9a64-473b-c303-f587453cc9b1"
      },
      "source": [
        "11260/11270, 6136/7503"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9991126885536823, 0.8178062108489937)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVz-joGmVzGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cf6e0b00-81bb-4b45-a39e-f035038715ef"
      },
      "source": [
        "model = SoftmaxMs(24888, 20)\n",
        "summary(model, (49776, 24888))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1           [-1, 49776, 200]       4,977,800\n",
            "              ReLU-2           [-1, 49776, 200]               0\n",
            "           Dropout-3           [-1, 49776, 200]               0\n",
            "            Linear-4           [-1, 49776, 100]          20,100\n",
            "              ReLU-5           [-1, 49776, 100]               0\n",
            "           Dropout-6           [-1, 49776, 100]               0\n",
            "            Linear-7            [-1, 49776, 20]           2,020\n",
            "           Sigmoid-8            [-1, 49776, 20]               0\n",
            "           Softmax-9            [-1, 49776, 20]               0\n",
            "================================================================\n",
            "Total params: 4,999,920\n",
            "Trainable params: 4,999,920\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4725.74\n",
            "Forward/backward pass size (MB): 364.57\n",
            "Params size (MB): 19.07\n",
            "Estimated Total Size (MB): 5109.39\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngy5GuMHwy4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxMess(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(SoftmaxMess, self).__init__()\n",
        "    self.batchnorm1 = torch.nn.BatchNorm1d(input_dim)\n",
        "    self.linear1 = torch.nn.Linear(input_dim, 500)\n",
        "    self.activ1 = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "    self.dropout2 = torch.nn.Dropout(0.5)\n",
        "    self.linear2 = torch.nn.Linear(500, 20)\n",
        "    self.activ2 = torch.nn.LogSigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.batchnorm1(x)\n",
        "    output = self.linear1(output)\n",
        "    output = self.activ1(output)\n",
        "\n",
        "    output = self.dropout2(output) \n",
        "    output = self.linear2(output)\n",
        "    output = self.activ2(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uy1pInjO7ZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "d9d06036-cef1-4fc8-88ec-57ab28d1c28e"
      },
      "source": [
        "model = SoftmaxMess(24888, 20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 51\n",
        "from time import time\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "  if torch.cuda.is_available():\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float().cuda())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform).cuda())\n",
        "  else:\n",
        "    X = Variable(torch.from_numpy(X_train_tfidf.toarray()).float())\n",
        "    Y = Variable(torch.from_numpy(Y_train_transform))\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  output = model(X)\n",
        "  loss = criterion(output, Y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"loss after {epoch} epochs: {loss}\")\n",
        "    pred1 = torch.argmax(model(torch.from_numpy(X_train_tfidf.toarray()).float().cuda()), axis=1)\n",
        "    s1 = sum(pred1 == torch.from_numpy(Y_train_transform).cuda())\n",
        "    print(f\"true train: {s1}\")\n",
        "\n",
        "    pred2 = torch.argmax(model(torch.from_numpy(X_test_tfidf.toarray()).float().cuda()), axis=1)\n",
        "    s2 = sum(pred2 == torch.from_numpy(Y_test_transform).cuda())\n",
        "    print(f\"true test: {s2}\")\n",
        "\n",
        "print(f\"training {epochs} epochs done after {time()-t1} s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after 0 epochs: 3.002722978591919\n",
            "true train: 10784\n",
            "true test: 5774\n",
            "loss after 10 epochs: 0.006617544684559107\n",
            "true train: 11241\n",
            "true test: 5892\n",
            "loss after 20 epochs: 0.004890852607786655\n",
            "true train: 11253\n",
            "true test: 5890\n",
            "loss after 30 epochs: 0.006686647888273001\n",
            "true train: 11257\n",
            "true test: 5949\n",
            "loss after 40 epochs: 0.004518006928265095\n",
            "true train: 11254\n",
            "true test: 5979\n",
            "loss after 50 epochs: 0.004200186114758253\n",
            "true train: 11261\n",
            "true test: 5948\n",
            "training 51 epochs done after 69.87499022483826 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwr3fTrDQT7Q",
        "colab_type": "text"
      },
      "source": [
        "## CNN with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwowio7bhQsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=1000)\n",
        "svd.fit(X_train_tfidf)\n",
        "X_train = svd.transform(X_train_tfidf)\n",
        "X_test = svd.transform(X_test_tfidf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXFeg1vCh1s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJB0iigtPBbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, n_class):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=24, kernel_size=5, stride=1, padding=2) # 24 input\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(in_channels=24, out_channels=32, kernel_size=3, stride=1, padding=1) # 32 input\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.maxpool1 = torch.nn.MaxPool1d(kernel_size=2, stride=2, padding=0) # 16 input\n",
        "        self.conv3 = torch.nn.Conv1d(in_channels=32, out_channels=48, kernel_size=3, stride=1, padding=1) # 24 input\n",
        "        self.maxpool2 = torch.nn.MaxPool1d(kernel_size=4, stride=4) # 8 input\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(input_dim * 6, out_features=n_class)\n",
        "        self.activ1 = torch.nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        # print(out.shape)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        # print(out.shape)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.conv3(out)\n",
        "        # print(out.shape)\n",
        "        out = self.maxpool2(out)\n",
        "        # print(out.shape)\n",
        "        out = self.fc1(out.view(out.shape[0], -1))\n",
        "        out = self.activ1(out)\n",
        "        \n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUjWESTGbCUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DataUtils(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x = torch.from_numpy(X_train).float()\n",
        "        self.y = torch.from_numpy(Y_train_transform)\n",
        "        self.length = self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):    \n",
        "        return self.x[index],self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "dataset = DataUtils()\n",
        "trainloader=DataLoader(dataset=dataset,batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Gs1XspbPFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = CNNModel(1000, 20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 101\n",
        "from time import time\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "    for X, Y in trainloader:\n",
        "        if torch.cuda.is_available():\n",
        "            X = Variable(X.view(X.shape[0], 1, X.shape[1]).cuda())\n",
        "            Y = Variable(Y.cuda())\n",
        "        else:\n",
        "            X = Variable(X.shape[0], 1, X.shape[1])\n",
        "            Y = Variable(Y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X)\n",
        "        loss = criterion(output, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 5 == 0:\n",
        "        # CUDA out of memory. ==>\n",
        "        print(f\"loss after {epoch} epochs: {loss}\")\n",
        "        pred1 = torch.argmax(model(torch.from_numpy(X_train[:1000]).view(X_train[:1000].shape[0], 1, X_train[:1000].shape[1]).float().cuda()), axis=1)\n",
        "        s1 = sum(pred1 == torch.from_numpy(Y_train_transform[:1000]).cuda())\n",
        "        print(f\"true train: {s1}\")\n",
        "\n",
        "        # pred2 = torch.argmax(model(torch.from_numpy(X_test).view(X_test.shape[0], 1, X_test.shape[1]).float().cuda()), axis=1)\n",
        "        # s2 = sum(pred2 == torch.from_numpy(Y_test_transform).cuda())\n",
        "        # print(f\"true test: {s2}\")\n",
        "\n",
        "print(f\"training {epochs} epochs done after {time()-t1} s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRf_9dxTX0vM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bbb7be8-b8df-4d90-ab1e-527d24158ea8"
      },
      "source": [
        "# CUDA out of memory. ==>\n",
        "batch_predict = 1000\n",
        "e_train = X_train.shape[0] // 1000\n",
        "e_test = X_test.shape[0] // 1000\n",
        "train_true = 0\n",
        "test_true = 0\n",
        "for i in range(e_train):\n",
        "    my_x = X_train[batch_predict * i: batch_predict*(i+1)]\n",
        "    pred1 = torch.argmax(model(torch.from_numpy(my_x).view(my_x.shape[0], 1, my_x.shape[1]).float().cuda()), axis=1)\n",
        "    s1 = sum(pred1 == torch.from_numpy(Y_train_transform[batch_predict * i: batch_predict*(i+1)]).cuda())\n",
        "    train_true += s1\n",
        "for i in range(e_test):    \n",
        "    my_x = X_test[batch_predict * i: batch_predict*(i+1)]\n",
        "    pred2 = torch.argmax(model(torch.from_numpy(my_x).view(my_x.shape[0], 1, my_x.shape[1]).float().cuda()), axis=1)\n",
        "    s2 = sum(pred2 == torch.from_numpy(Y_test_transform[batch_predict * i: batch_predict*(i+1)]).cuda())\n",
        "    test_true += s2\n",
        "\n",
        "print(f\"true train: {train_true} / {X_train.shape[0]}\")\n",
        "print(f\"true test: {test_true} / {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true train: 10269 / 11270\n",
            "true test: 4699 / 7503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm3L4S87pNmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02mKeV0tro3",
        "colab_type": "text"
      },
      "source": [
        "## RNN with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFkIsoVwWj5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a5d7addd-8e52-4767-d0dd-d8d0e53d6309"
      },
      "source": [
        "x = torch.randint(0, 100, (3, 8))\n",
        "print(x)\n",
        "embedding = torch.nn.Embedding(100, 5) # vocab size = 100 so input to embedding layer < 100\n",
        "print(embedding(x).shape) # [3, 8, 5] \n",
        "# 3x8 -> embedding -> 3x8x5 -> LSTM (with hidden size=3)-> 3x3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 5, 87, 41, 15, 71, 40, 83, 49],\n",
            "        [82, 40, 57, 46, 30, 55,  5, 70],\n",
            "        [80, 56, 90, 10, 11, 50, 28, 42]])\n",
            "torch.Size([3, 8, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSNE-Dt3a9ll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f4e908f4-ffc0-4760-b926-031057da2c0f"
      },
      "source": [
        "a = torch.randint(0, 5, (2, 3))\n",
        "ebd = torch.nn.Embedding(10, 2)\n",
        "print(a)\n",
        "print(ebd(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3, 2, 4],\n",
            "        [2, 0, 1]])\n",
            "tensor([[[ 0.1041,  0.1994],\n",
            "         [ 0.2279,  0.6745],\n",
            "         [-0.1467, -0.4252]],\n",
            "\n",
            "        [[ 0.2279,  0.6745],\n",
            "         [ 0.6440,  1.5220],\n",
            "         [-0.2190,  0.3437]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjvLfSIJGKaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.RNN(10, 20, 2) # (input_size, hidden_size, num_layer)\n",
        "input = torch.randn(5, 3, 10) # (seq_len, batch, input_size) # input_size: kch thc 1 vector xt, seq_len: nof time step\n",
        "h0 = torch.randn(2, 3, 20) # (num_layers * num_directions, batch, hidden_size)\n",
        "# initial hidden state for each element in the batch. Defaults to zero if not provided\n",
        "\n",
        "# output: (seq_len, batch, num_directions * hidden_size)\n",
        "# hn: (num_layers * num_directions, batch, hidden_size)\n",
        "output, hn = rnn(input, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DShxA32MGZr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = [len(x) for x in X_train_count_idx]\n",
        "length1 = [len(x) for x in X_test_count_idx]\n",
        "np.mean(length) # 84.294942324756\n",
        "np.mean(length1) # 82.27535652405705\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3sRiw9zNBNM",
        "colab_type": "text"
      },
      "source": [
        "#### prepare data for RNN naive approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFm0nDwDK26A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorize_tfidf.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP67Tix_h8CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputUtils():\n",
        "    def __init__(self, glove_file=\"\"):\n",
        "        self.word_vectors = self.load_glove_vectors()\n",
        "\n",
        "    def load_glove_vectors(self, glove_file=\"/content/drive/My Drive/glove.6B.50d.txt\"):\n",
        "        \"\"\"Load the glove word vectors\"\"\"\n",
        "        glove_word_vectors = {}\n",
        "        print(glove_file)\n",
        "        with open(glove_file) as f:\n",
        "            for line in f:\n",
        "                split = line.split()\n",
        "                glove_word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
        "        return glove_word_vectors\n",
        "\n",
        "    # @staticmethod\n",
        "    def create_list_word_vector_from_sentence(self, sentence, sen_size): # todo: init result: zeros => set value=> faster than concatenate\n",
        "        list_word = sentence.split(\" \")\n",
        "        matrix = np.zeros((sen_size, 50)) # padding = 0\n",
        "        n = len(list_word)\n",
        "        for i in range(min(n, 80)):\n",
        "            matrix[i] = np.array(self.word_vectors.get(list_word[i], np.zeros((50,))).reshape(1, -1))\n",
        "\n",
        "        assert matrix.shape[0] == 80, f\"Output not match shape {matrix.shape}\"\n",
        "        return matrix\n",
        "    \n",
        "    # @staticmethod\n",
        "    def create_input_from_corpus(self, list_sentence):\n",
        "        n = len(list_sentence)\n",
        "        output = np.zeros((n, 80, 50))\n",
        "        for i in range(n):\n",
        "            output[i] = self.create_list_word_vector_from_sentence(list_sentence[i], 80)\n",
        "        assert output.shape == (len(list_sentence), 80, 50), f\"Output not match shape {output.shape}\"\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z9Cd3Q1Q1KS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fa30be2-fd47-48f8-bdd3-9dbe2fc2e382"
      },
      "source": [
        "# list_sentence = ['hello my word', \"how are you now\"]\n",
        "util = InputUtils(\"\")\n",
        "# output = ut.create_input_from_corpus(list_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/glove.6B.50d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4Sn31idph9",
        "colab_type": "text"
      },
      "source": [
        "#### Test np reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6RIlgabdFz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "8ef14f28-14d6-4195-fd5f-20ad20f809be"
      },
      "source": [
        "np.random.seed(68)\n",
        "a = np.random.randint(0, 10, (3, 4, 5))\n",
        "x, y, z = a.shape\n",
        "b = np.zeros((y, x, z))\n",
        "\n",
        "for i in range(a.shape[0]):\n",
        "    b[:, i, :] = a[i]\n",
        "\n",
        "print(a)\n",
        "print('-------------------')\n",
        "print(b)\n",
        "print('-------------------')\n",
        "print(a[0])\n",
        "print('-------------------')\n",
        "print(b[:, 0, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[2 7 4 0 8]\n",
            "  [7 4 6 3 3]\n",
            "  [8 5 1 9 8]\n",
            "  [6 1 4 5 0]]\n",
            "\n",
            " [[7 3 5 4 7]\n",
            "  [3 7 2 5 2]\n",
            "  [9 8 0 9 0]\n",
            "  [5 7 2 9 4]]\n",
            "\n",
            " [[4 2 2 7 7]\n",
            "  [6 0 9 1 1]\n",
            "  [0 4 3 4 7]\n",
            "  [2 5 2 6 3]]]\n",
            "-------------------\n",
            "[[[2. 7. 4. 0. 8.]\n",
            "  [7. 3. 5. 4. 7.]\n",
            "  [4. 2. 2. 7. 7.]]\n",
            "\n",
            " [[7. 4. 6. 3. 3.]\n",
            "  [3. 7. 2. 5. 2.]\n",
            "  [6. 0. 9. 1. 1.]]\n",
            "\n",
            " [[8. 5. 1. 9. 8.]\n",
            "  [9. 8. 0. 9. 0.]\n",
            "  [0. 4. 3. 4. 7.]]\n",
            "\n",
            " [[6. 1. 4. 5. 0.]\n",
            "  [5. 7. 2. 9. 4.]\n",
            "  [2. 5. 2. 6. 3.]]]\n",
            "-------------------\n",
            "[[2 7 4 0 8]\n",
            " [7 4 6 3 3]\n",
            " [8 5 1 9 8]\n",
            " [6 1 4 5 0]]\n",
            "-------------------\n",
            "[[2. 7. 4. 0. 8.]\n",
            " [7. 4. 6. 3. 3.]\n",
            " [8. 5. 1. 9. 8.]\n",
            " [6. 1. 4. 5. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bUQXqrWgzCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj6ZccOsUA-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b249465-218e-4e39-d143-39c423c983b6"
      },
      "source": [
        "start = time()\n",
        "matrix_train = util.create_input_from_corpus(X_train)\n",
        "end = time()\n",
        "print(f\"Prepare data for RNN done after {end - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepare data for RNN done after 3.184373378753662s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3E6ePAen2JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11dbdd69-ff31-43ad-e95e-d214b56a1992"
      },
      "source": [
        "start = time()\n",
        "matrix_test = util.create_input_from_corpus(X_test)\n",
        "end = time()\n",
        "print(f\"Prepare data for RNN done after {end - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepare data for RNN done after 2.4487249851226807s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ7sLLlrqxWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "629f79a2-6c97-4e24-efda-77f1a4659d4a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.41037 ,  0.11342 ,  0.051524, ..., -0.36064 , -0.19616 ,\n",
              "        -0.81066 ],\n",
              "       [ 0.4786  ,  0.27493 , -1.2158  , ..., -0.93431 , -0.14269 ,\n",
              "        -0.085625],\n",
              "       [ 0.90512 ,  0.13008 ,  1.0917  , ...,  0.62759 ,  0.34694 ,\n",
              "         0.46287 ],\n",
              "       ...,\n",
              "       [ 0.61183 , -0.22072 , -0.10898 , ..., -0.043688, -0.097922,\n",
              "         0.16806 ],\n",
              "       [ 1.0802  ,  0.085736,  0.28167 , ...,  0.062295,  0.16278 ,\n",
              "         0.17376 ],\n",
              "       [-0.66987 , -0.52013 ,  0.52627 , ...,  0.32978 ,  0.78675 ,\n",
              "        -1.217   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TquNJMHzr1WQ",
        "colab_type": "text"
      },
      "source": [
        "#### RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMw-mErSrot7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class RNNModel(torch.nn.Module):\n",
        "#     def __init__(self, input_size, output_size, hidden_size, num_layer, batch_size):\n",
        "#         super(RNNModel, self).__init__()\n",
        "\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_layer = num_layer\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         self.rnn = torch.nn.RNN(input_size, hidden_size, num_layer, batch_first=True)\n",
        "#         self.softmax1 = torch.nn.Softmax(dim=1)   \n",
        "#         self.fc = torch.nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         hidden = torch.randn(self.num_layer, self.batch_size, self.hidden_size)\n",
        "#         out, hidden = self.rnn(x, hidden)\n",
        "#         out = out.contiguous().view(-1, self.hidden_size) # contiguous(): make a copy of out just ff\n",
        "#         out = self.fc(out)\n",
        "#         out = self.softmax1(out)\n",
        "#         return out, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRRkaf-r-Rgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNMd(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layer, batch, output_size):\n",
        "        super(RNNMd, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.batch = batch\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layer, batch_first=True)\n",
        "        self.softmax1 = torch.nn.Softmax(dim=1)   \n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
        "        print(\"init model done!\")\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: batch\n",
        "        \"\"\"\n",
        "        # batch = x.size\n",
        "        # batch_ = x.shape[1] # conflict name rs \n",
        "        # print(\"in forward\")\n",
        "        h0 = torch.zeros(self.num_layer, x.shape[1], self.hidden_size) # init\n",
        "        # if torch.cuda.is_available():\n",
        "        #     h0.cuda()\n",
        "        # print(\"computing output done!\")\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        # checkout hn[-1] here instead of out[-1]\n",
        "        out = hn[-1].contiguous().view(-1, self.hidden_size) # contiguous(): make a copy of out just ff, take last output only\n",
        "        # out = self.fc(out) \n",
        "        out = self.softmax1(out)\n",
        "        # print(\"done forward!\")\n",
        "        return out, hn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl6hRIL5_31Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_size, hidden_size, num_layer, batch, output_size = 50, 20, 1, 8, 20\n",
        "# rnn = torch.nn.RNN(10, 20, 2) # (input_size, hidden_size, num_layer)\n",
        "# input = torch.randn(5, 3, 10) # (seq_len, batch, input_size) # input_size: kch thc 1 vector xt, seq_len: nof time step\n",
        "# h0 = torch.randn(2, 3, 20) # (num_layers * num_directions, batch, hidden_size)\n",
        "# # initial hidden state for each element in the batch. Defaults to zero if not provided\n",
        "\n",
        "# # output: (seq_len, batch, num_directions * hidden_size)\n",
        "# # hn: (num_layers * num_directions, batch, hidden_size)\n",
        "# output, hn = rnn(input, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L6pvLz1AyOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_size, hidden_size, num_layer, batch, output_size = 50, 20, 1, 8, 20\n",
        "# model = RNNMd(input_size, hidden_size, num_layer, batch, output_size)\n",
        "\n",
        "# output, hn = model(torch.randn(80, batch, input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkexSTAicBpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_train = np.random.randn(100, 80, 50)\n",
        "Y_train = np.random.randint(0, 99, (100,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NleU76AgCAYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DataUtils(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        # x, y, z = X.shape # not reshape like bro thought\n",
        "        # X_reshape = np.zeros((y, x, z))\n",
        "        # for i in range(X.shape[0]):\n",
        "        #     X_reshape[:, i, :] = X[i]\n",
        "\n",
        "        self.x = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(Y)\n",
        "        self.length = self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):    \n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "dataset = DataUtils(matrix_train, Y_train)\n",
        "trainloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p4dezl9OVJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de43d5a0-b15f-4418-ef7b-f2093f79f4a7"
      },
      "source": [
        "matrix_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11270, 80, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sba7gQ3B3vhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e5f650d1-69f4-41f1-a398-f4ce20caba4c"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "input_size, hidden_size, num_layer, batch, output_size = 50, 40, 2, 128, 20\n",
        "model = RNNMd(input_size, hidden_size, num_layer, batch, output_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    pass\n",
        "    # model.cuda()\n",
        "\n",
        "print(\"done init model cuda\")\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 11\n",
        "from time import time\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "    for X, Y in trainloader:\n",
        "        # x, y, z = X.shape # not reshape like bro thought\n",
        "        # X_reshape = np.zeros((y, x, z))\n",
        "\n",
        "        # for i in range(X.shape[0]):\n",
        "        #     X_reshape[:, i, :] = X[i]\n",
        "\n",
        "        X_reshape = torch.from_numpy(X_reshape).float()\n",
        "        # if torch.cuda.is_available():\n",
        "        #     # X = Variable(X.view(X.shape[1], X.shape[0], X.shape[2]).cuda())\n",
        "        #     X = Variable(X_reshape)\n",
        "        #     # Y = Variable(Y.cuda())\n",
        "        #     Y = Variable(Y)\n",
        "        # else:\n",
        "        X = Variable(X)\n",
        "        Y = Variable(Y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output, hn = model(X)\n",
        "        # print(f\"shape of X: {X.shape}\")\n",
        "        # print(Y.shape)\n",
        "        # print(X_reshape.shape)\n",
        "        # print(output.shape)\n",
        "        # break\n",
        "        loss = criterion(output, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 2 == 0:\n",
        "        # CUDA out of memory. ==>\n",
        "        print(f\"loss after {epoch} epochs: {loss}\") # todo: test from here\n",
        "        # pred1 = torch.argmax(model(torch.from_numpy(X_train[:1000]).view(X_train[:1000].shape[1], X_train[0], X_train[:1000].shape[2]).float().cuda()), axis=1)\n",
        "        # s1 = sum(pred1 == torch.from_numpy(Y_train[:1000]).cuda())\n",
        "        # print(f\"true train: {s1}\")\n",
        "\n",
        "        # pred2 = torch.argmax(model(torch.from_numpy(X_test).view(X_test.shape[0], 1, X_test.shape[1]).float().cuda()), axis=1)\n",
        "        # s2 = sum(pred2 == torch.from_numpy(Y_test_transform).cuda())\n",
        "        # print(f\"true test: {s2}\")\n",
        "\n",
        "print(f\"training {epochs} epochs done after {time()-t1} s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init model done!\n",
            "done init model cuda\n",
            "loss after 0 epochs: 3.673224687576294\n",
            "loss after 2 epochs: 3.6643974781036377\n",
            "loss after 4 epochs: 3.6786844730377197\n",
            "loss after 6 epochs: 3.6676483154296875\n",
            "loss after 8 epochs: 3.6648311614990234\n",
            "loss after 10 epochs: 3.6620638370513916\n",
            "training 11 epochs done after 67.64333820343018 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3DE_SLlnYk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = 0\n",
        "for X, Y in trainloader:\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "    x, y, z = X.shape # not reshape like bro thought\n",
        "    X_reshape = np.zeros((y, x, z))\n",
        "    for i in range(a.shape[0]):\n",
        "        X_reshape[:, i, :] = X[i]\n",
        "    print(X_reshape.shape)\n",
        "    break\n",
        "    X = Variable(X.view(X.shape[1], X.shape[0], X.shape[2]))\n",
        "    Y = Variable(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    # print(model(X))\n",
        "    break\n",
        "    pred1 = torch.argmax(model(X), axis=1)\n",
        "    s1 = sum(pred1 == torch.from_numpy(Y))\n",
        "    s += s1\n",
        "    print(f\"true train: {s1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtlNuMAyRsHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86565653-03ff-4490-f980-894140249d7b"
      },
      "source": [
        "s1 = 0\n",
        "for X, Y in trainloader:\n",
        "        if torch.cuda.is_available():\n",
        "            # X = Variable(X.view(X.shape[1], X.shape[0], X.shape[2]).cuda())\n",
        "            X = Variable(X.view(X.shape[1], X.shape[0], X.shape[2]))\n",
        "            # Y = Variable(Y.cuda())\n",
        "            Y = Variable(Y)\n",
        "        else:\n",
        "            X = Variable(X.shape[1], X.shape[0], X.shape[2])\n",
        "            Y = Variable(Y)\n",
        "        output, hn = model(X)\n",
        "        pred1 = torch.argmax(output, axis=1)\n",
        "        s1 += sum(pred1 == Y)\n",
        "        # print(f\"true train: {s1}\")\n",
        "s1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(561)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC1-mHevTM7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91b1579b-361c-4ad7-a3ae-4cd63bafd734"
      },
      "source": [
        "s1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1346)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8xuJbBzTDED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfbd79d0-7c0d-4194-d4dd-30da2c047be2"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpJbSbmBELOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install -q --upgrade prompt-toolkit\n",
        "!pip install -q --upgrade ipython\n",
        "!pip install -q --upgrade ipykernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilnylrF6edQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GnsFoSmvwpp",
        "colab_type": "text"
      },
      "source": [
        "*Another cause - if you're using PyTorch and assign your model to the GPU, but don't assign an internal tensor to the GPU (e.g. a hidden layer).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DesRijPuz6N5",
        "colab_type": "text"
      },
      "source": [
        "*If you implement your own nn.Module, which includes some parameters inside. You should declare it as Parameters to let nn.Module.cuda() transfer the parameters into GPU memory.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah-cj74v0n7l",
        "colab_type": "text"
      },
      "source": [
        "Keep in mind that only a limited number of optimizers support sparse gradients: currently its optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGbSCiJ2v-1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW1Qct8Y1BQ_",
        "colab_type": "text"
      },
      "source": [
        "### RNN with embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-FHhm7-zbZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn.Embedding(num_embedding, embedding_dims,*): \n",
        "# A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "# x: matrix input 3*8\n",
        "# y: embedding matrix 3*8*7\n",
        "# if:\n",
        "#     x[0, 4]==x[0, 5]==x[1, 4]\n",
        "# then:\n",
        "#     y[0, 4]==y[0, 5]==y[1, 4]\n",
        "# same word => same embedding vector\n",
        "\n",
        "# y[i, j] => embedding vector for word x[i, j]\n",
        "# input: Input: (*)() , LongTensor of arbitrary shape containing the indices to extract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9CtXojp1i7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dict(corpus):\n",
        "    coropa = {\" \", \"US0\"} # \" \": padding, US0: unseen word\n",
        "    for sentence in corpus:\n",
        "        for word in sentence.split():\n",
        "            coropa.add(word)\n",
        "\n",
        "    coropa = list(coropa)\n",
        "\n",
        "    map_word = dict()\n",
        "    for i, v in enumerate(coropa):\n",
        "        map_word[v] = i\n",
        "    return map_word\n",
        "\n",
        "word2idx = make_dict(X_train) # 70739"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHpXoPaL3ETm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean length: 80\n",
        "def corpus2vec(corpus, word2idx, sen_size):\n",
        "    def sen2vec(sentence):\n",
        "        s = sentence.split()\n",
        "        tmp = list(map(lambda x: word2idx.get(x, word2idx.get(\"US0\")), s))\n",
        "        n = len(tmp)\n",
        "        if n < sen_size:\n",
        "            tmp += [word2idx[\" \"]] * (sen_size-n)\n",
        "        \n",
        "        vector_sentence = np.array(tmp[:80])\n",
        "        return vector_sentence\n",
        "    \n",
        "    n = len(corpus)\n",
        "    sentence_matrix = np.zeros((n, sen_size))\n",
        "\n",
        "    for i in range(n):\n",
        "        sentence_matrix[i] = sen2vec(corpus[i])\n",
        "    return sentence_matrix\n",
        "\n",
        "sentence_matrix = corpus2vec(X_train, word2idx, 80)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z87_HQH6StS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataUtils(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]).long(), self.y[idx]\n",
        "\n",
        "dataset = DataUtils(sentence_matrix, Y_train)\n",
        "trainloader = DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tji--o2DJJOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_matrix = corpus2vec(X_test, word2idx, 80)\n",
        "\n",
        "testset = DataUtils(test_matrix, Y_test)\n",
        "testloader = DataLoader(dataset=testset, batch_size=256, shuffle=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW5yVuzF9by7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNEbd(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, ebd_dim, num_layers, hidden_size, output_size):\n",
        "        super(RNNEbd, self).__init__()\n",
        "        self.embeddings = torch.nn.Embedding(vocab_size, ebd_dim) # add pad before\n",
        "        self.lstm = torch.nn.LSTM(ebd_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.4)\n",
        "        # change\n",
        "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
        "        # self.linear2 = torch.nn.Linear(20, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        output = self.embeddings(x)\n",
        "        output = self.dropout(output)\n",
        "        # output, hn = self.rnn(output)\n",
        "        lstm_out, (ht, ct) = self.lstm(output)\n",
        "        output = self.linear(ht[-1])\n",
        "        # output = self.linear2(output)\n",
        "        output = self.softmax(output)\n",
        "        return output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7bK67ECOEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c07ffa7-14ed-4501-f0cb-accc98f61623"
      },
      "source": [
        "model = RNNEbd(len(word2idx), 300, 1, 50, 20)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "epochs = 101\n",
        "from time import time\n",
        "t1 = time()\n",
        "for epoch in range(epochs):\n",
        "    for X, Y in trainloader:\n",
        "        X = X.long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 5 == 0:\n",
        "        # CUDA out of memory. ==>\n",
        "        print(f\"loss after {epoch} epochs: {loss}\") # todo: test from here\n",
        "        s = 0\n",
        "        for X, Y in trainloader:\n",
        "            X = X.long()\n",
        "            output = model(X)\n",
        "            pred = torch.argmax(output, axis=1)\n",
        "            tmp = torch.sum(pred==Y)\n",
        "            s += tmp\n",
        "        print(s)\n",
        "        s = 0\n",
        "        for X, Y in testloader:\n",
        "            X = X.long()\n",
        "            output = model(X)\n",
        "            pred = torch.argmax(output, axis=1)\n",
        "            tmp = torch.sum(pred==Y)\n",
        "            s += tmp\n",
        "        print(s)\n",
        "\n",
        "print(f\"training {epochs} epochs done after {time()-t1} s\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after 0 epochs: 2.997466802597046\n",
            "tensor(1265)\n",
            "tensor(580)\n",
            "loss after 5 epochs: 2.585172653198242\n",
            "tensor(3804)\n",
            "tensor(1590)\n",
            "loss after 10 epochs: 2.8502063751220703\n",
            "tensor(5622)\n",
            "tensor(2356)\n",
            "loss after 15 epochs: 2.8906688690185547\n",
            "tensor(7022)\n",
            "tensor(2959)\n",
            "loss after 20 epochs: 2.4569685459136963\n",
            "tensor(8200)\n",
            "tensor(3514)\n",
            "loss after 25 epochs: 2.4102141857147217\n",
            "tensor(8872)\n",
            "tensor(3983)\n",
            "loss after 30 epochs: 2.2513835430145264\n",
            "tensor(9220)\n",
            "tensor(4134)\n",
            "loss after 35 epochs: 2.568661689758301\n",
            "tensor(9725)\n",
            "tensor(4286)\n",
            "loss after 40 epochs: 2.269740343093872\n",
            "tensor(9768)\n",
            "tensor(4336)\n",
            "loss after 45 epochs: 2.2450950145721436\n",
            "tensor(10058)\n",
            "tensor(4493)\n",
            "loss after 50 epochs: 2.0784685611724854\n",
            "tensor(10246)\n",
            "tensor(4684)\n",
            "loss after 55 epochs: 2.1063005924224854\n",
            "tensor(10367)\n",
            "tensor(4620)\n",
            "loss after 60 epochs: 2.241914749145508\n",
            "tensor(10470)\n",
            "tensor(4709)\n",
            "loss after 65 epochs: 2.0905604362487793\n",
            "tensor(10517)\n",
            "tensor(4772)\n",
            "loss after 70 epochs: 2.2492287158966064\n",
            "tensor(10557)\n",
            "tensor(4845)\n",
            "loss after 75 epochs: 2.2470428943634033\n",
            "tensor(10614)\n",
            "tensor(4868)\n",
            "loss after 80 epochs: 2.086588144302368\n",
            "tensor(10629)\n",
            "tensor(4882)\n",
            "loss after 85 epochs: 2.1642377376556396\n",
            "tensor(10643)\n",
            "tensor(4909)\n",
            "loss after 90 epochs: 2.0781590938568115\n",
            "tensor(10686)\n",
            "tensor(4944)\n",
            "loss after 95 epochs: 2.5495612621307373\n",
            "tensor(10711)\n",
            "tensor(4970)\n",
            "loss after 100 epochs: 2.244429588317871\n",
            "tensor(10723)\n",
            "tensor(5073)\n",
            "training 101 epochs done after 1867.1509721279144 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH7Nq6SfWI6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss after 0 epochs: 2.982945442199707\n",
        "# tensor(1498)\n",
        "# tensor(665)\n",
        "# loss after 5 epochs: 2.8098628520965576\n",
        "# tensor(4826)\n",
        "# tensor(1834)\n",
        "# loss after 10 epochs: 2.4296891689300537\n",
        "# tensor(6983)\n",
        "# tensor(2891)\n",
        "# loss after 15 epochs: 2.578169584274292\n",
        "# tensor(8144)\n",
        "# tensor(3437)\n",
        "# loss after 20 epochs: 2.37660813331604\n",
        "# tensor(8720)\n",
        "# tensor(3770)\n",
        "# loss after 25 epochs: 2.4084534645080566\n",
        "# tensor(9064)\n",
        "# tensor(3957)\n",
        "# loss after 30 epochs: 2.1120564937591553\n",
        "# tensor(9524)\n",
        "# tensor(4178)\n",
        "# loss after 35 epochs: 2.0857508182525635\n",
        "# tensor(9887)\n",
        "# tensor(4285)\n",
        "# loss after 40 epochs: 2.1238152980804443\n",
        "# tensor(10193)\n",
        "# tensor(4438)\n",
        "# loss after 45 epochs: 2.1881587505340576\n",
        "# tensor(10356)\n",
        "# tensor(4546)\n",
        "# loss after 50 epochs: 2.0784521102905273\n",
        "# tensor(10489)\n",
        "# tensor(4578)\n",
        "# loss after 55 epochs: 2.07915997505188\n",
        "# tensor(10564)\n",
        "# tensor(4662)\n",
        "# loss after 60 epochs: 2.0783541202545166\n",
        "# tensor(10623)\n",
        "# tensor(4745)\n",
        "# loss after 65 epochs: 2.0781590938568115\n",
        "# tensor(10652)\n",
        "# tensor(4779)\n",
        "# loss after 70 epochs: 2.078335762023926\n",
        "# tensor(10670)\n",
        "# tensor(4802)\n",
        "# loss after 75 epochs: 2.078561544418335\n",
        "# tensor(10694)\n",
        "# tensor(4849)\n",
        "# loss after 80 epochs: 2.0781846046447754\n",
        "# tensor(10706)\n",
        "# tensor(4849)\n",
        "# loss after 85 epochs: 2.0872957706451416\n",
        "# tensor(10725)\n",
        "# tensor(4892)\n",
        "# loss after 90 epochs: 2.0781571865081787\n",
        "# tensor(10734)\n",
        "# tensor(4915)\n",
        "# loss after 95 epochs: 2.1022636890411377\n",
        "# tensor(10727)\n",
        "# tensor(4881)\n",
        "# loss after 100 epochs: 2.244936227798462\n",
        "# tensor(10762)\n",
        "# tensor(4856)\n",
        "# training 101 epochs done after 2350.489052772522 s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VUJPJWFLOa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f7571d1-a7f1-474f-aa57-e56c20ee7a67"
      },
      "source": [
        "s = 0\n",
        "for X, Y in testloader:\n",
        "    X = X.long()\n",
        "    output = model(X)\n",
        "    pred = torch.argmax(output, axis=1)\n",
        "    tmp = torch.sum(pred==Y)\n",
        "    s += tmp\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4169)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4JrVQXlFL-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ff22415-0399-447d-c894-498dbc8ed53b"
      },
      "source": [
        "Y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7503,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKySjHHoMcy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}